{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454ab2fd",
   "metadata": {},
   "source": [
    "# PySpark DataFrame Reference Guide\n",
    "\n",
    "This notebook serves as a comprehensive reference for working with Spark DataFrames in PySpark.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Spark Session Initialization](#spark-session)\n",
    "2. [Creating DataFrames](#creating-dataframes)\n",
    "3. [Reading Data from Files](#reading-files)\n",
    "4. [DataFrame Operations](#dataframe-operations)\n",
    "5. [Selecting and Filtering Data](#selecting-filtering)\n",
    "6. [Aggregations and Grouping](#aggregations)\n",
    "7. [Schema Definition](#schema)\n",
    "8. [String Operations](#string-operations)\n",
    "9. [Performance and Partitioning](#performance)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bed018f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30566c8",
   "metadata": {},
   "source": [
    "## 1. Spark Session Initialization <a name=\"spark-session\"></a>\n",
    "\n",
    "The SparkSession is the entry point for all Spark functionality. It's the first thing you need to create when working with Spark DataFrames.\n",
    "\n",
    "**Key Points:**\n",
    "- `SparkSession.builder` creates a new session or retrieves an existing one\n",
    "- `appName()` sets the application name (visible in Spark UI)\n",
    "- `config()` sets Spark configuration options\n",
    "- `getOrCreate()` creates a new session or gets the existing one\n",
    "- `setLogLevel()` controls logging verbosity (ERROR, WARN, INFO, DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fde2e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|    name|\n",
      "+---+--------+\n",
      "| 30| John D.|\n",
      "| 25|Alice G.|\n",
      "| 35|  Bob T.|\n",
      "| 28|  Eve A.|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = [{\"name\": \"John D.\", \"age\": 30},\n",
    "  {\"name\": \"Alice G.\", \"age\": 25},\n",
    "  {\"name\": \"Bob T.\", \"age\": 35},\n",
    "  {\"name\": \"Eve A.\", \"age\": 28}]\n",
    "\n",
    "# Create a DataFrame containing the employees data\n",
    "df = spark.createDataFrame(employees)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990eb7c8",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames <a name=\"creating-dataframes\"></a>\n",
    "\n",
    "### Method 1: From Python Collections\n",
    "\n",
    "You can create DataFrames directly from Python lists, dictionaries, or tuples.\n",
    "\n",
    "**Common Methods:**\n",
    "- `spark.createDataFrame(data)` - Creates DataFrame from Python collections\n",
    "- `df.show()` - Displays the DataFrame content\n",
    "- `df.show(n, truncate=False)` - Shows n rows without truncating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fe0560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John D.', 30)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = df.collect()[0]\n",
    "r1.name, r1.age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533bceb",
   "metadata": {},
   "source": [
    "### Accessing DataFrame Rows\n",
    "\n",
    "**Key Methods:**\n",
    "- `df.collect()` - Returns all rows as a list of Row objects (⚠️ Use with caution on large datasets!)\n",
    "- `df.first()` - Returns the first row\n",
    "- `df.take(n)` - Returns first n rows\n",
    "- Row objects allow attribute-style access: `row.columnName`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c313e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    name|avg(age)|\n",
      "+--------+--------+\n",
      "| John D.|    30.0|\n",
      "|Alice G.|    25.0|\n",
      "|  Bob T.|    35.0|\n",
      "|  Eve A.|    28.0|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_age = df.groupBy('name').avg('age')\n",
    "avg_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12797d",
   "metadata": {},
   "source": [
    "## 3. Aggregations and Grouping <a name=\"aggregations\"></a>\n",
    "\n",
    "### Basic Aggregation (Simple Method)\n",
    "\n",
    "The simple `groupBy().avg()` syntax is quick but less flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29a20a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53243c81",
   "metadata": {},
   "source": [
    "## 4. Performance and Partitioning <a name=\"performance\"></a>\n",
    "\n",
    "### Understanding Partitions\n",
    "\n",
    "Partitions determine how data is distributed across the cluster. Understanding partitioning is crucial for performance optimization.\n",
    "\n",
    "**Key Concepts:**\n",
    "- DataFrames are divided into partitions for parallel processing\n",
    "- Number of partitions affects parallelism and performance\n",
    "- Use `rdd.getNumPartitions()` to check partition count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22cb9eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_age.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6e40612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- average_age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_age = df.groupBy('name').agg(avg('age').alias('average_age'))\n",
    "\n",
    "avg_age.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d15f7",
   "metadata": {},
   "source": [
    "### Advanced Aggregation with agg()\n",
    "\n",
    "The `agg()` method is more powerful and flexible than simple aggregation methods.\n",
    "\n",
    "**Benefits:**\n",
    "- Can apply multiple aggregations at once\n",
    "- Allows custom column naming with `alias()`\n",
    "- Works with any Spark SQL function\n",
    "- More readable and maintainable code\n",
    "\n",
    "**Common Aggregation Functions:**\n",
    "- `F.avg()` - Average\n",
    "- `F.count()` - Count\n",
    "- `F.sum()` - Sum\n",
    "- `F.min()` / `F.max()` - Min/Max\n",
    "- `F.stddev()` - Standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95d8c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    name|average_age|\n",
      "+--------+-----------+\n",
      "| John D.|       30.0|\n",
      "|Alice G.|       25.0|\n",
      "|  Bob T.|       35.0|\n",
      "|  Eve A.|       28.0|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69ed4fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"people.json\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9320343",
   "metadata": {},
   "source": [
    "## 5. Reading Data from Files <a name=\"reading-files\"></a>\n",
    "\n",
    "### Reading JSON Files\n",
    "\n",
    "Spark can automatically infer schema from JSON files.\n",
    "\n",
    "**Common Options:**\n",
    "- `spark.read.json(path)` - Reads JSON with schema inference\n",
    "- `spark.read.json(path, schema=mySchema)` - Reads with explicit schema\n",
    "- `df.printSchema()` - Displays the DataFrame schema in tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b705417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|age |name   |\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|30  |Andy   |\n",
      "|19  |Justin |\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe95df35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d98fd",
   "metadata": {},
   "source": [
    "## 6. Selecting and Filtering Data <a name=\"selecting-filtering\"></a>\n",
    "\n",
    "### Column Selection Methods\n",
    "\n",
    "There are multiple ways to select columns in PySpark:\n",
    "\n",
    "**Method 1:** Dictionary-style (returns Column object)\n",
    "```python\n",
    "df['columnName']\n",
    "```\n",
    "\n",
    "**Method 2:** Using F.col() (recommended for complex operations)\n",
    "```python\n",
    "F.col(\"columnName\")\n",
    "```\n",
    "\n",
    "**Method 3:** Using select() with string (most common)\n",
    "```python\n",
    "df.select(\"columnName\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57fe0880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.select(F.col(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "862c7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_names = df.select(\"name\")\n",
    "df_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c960b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|NULL|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_age = df.select(\"age\")\n",
    "df_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9b256e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 30|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_adults= df_age.where(F.col(\"age\") >= 18)\n",
    "df_adults.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975ccb9",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "\n",
    "**Filter Methods:**\n",
    "- `df.where(condition)` - Filters rows based on condition\n",
    "- `df.filter(condition)` - Same as where() (alternative syntax)\n",
    "\n",
    "**Common Operators:**\n",
    "- `==`, `!=` - Equality/Inequality\n",
    "- `>`, `>=`, `<`, `<=` - Comparison\n",
    "- `&` - AND (use with parentheses!)\n",
    "- `|` - OR (use with parentheses!)\n",
    "- `~` - NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a4c32d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   name|average_age|\n",
      "+-------+-----------+\n",
      "|Michael|       NULL|\n",
      "|   Andy|       30.0|\n",
      "| Justin|       19.0|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+-----+-------+-------+\n",
      "|   name|count|avg_age|max_age|\n",
      "+-------+-----+-------+-------+\n",
      "|Michael|    1|   NULL|   NULL|\n",
      "|   Andy|    1|   30.0|     30|\n",
      "| Justin|    1|   19.0|     19|\n",
      "+-------+-----+-------+-------+\n",
      "\n",
      "+-------+-----+-------+-------+\n",
      "|   name|count|avg_age|max_age|\n",
      "+-------+-----+-------+-------+\n",
      "|Michael|    1|   NULL|   NULL|\n",
      "|   Andy|    1|   30.0|     30|\n",
      "| Justin|    1|   19.0|     19|\n",
      "+-------+-----+-------+-------+\n",
      "\n",
      "+------------+---------------+\n",
      "|total_people|overall_avg_age|\n",
      "+------------+---------------+\n",
      "|           3|           24.5|\n",
      "+------------+---------------+\n",
      "\n",
      "+------------+---------------+\n",
      "|total_people|overall_avg_age|\n",
      "+------------+---------------+\n",
      "|           3|           24.5|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Single aggregation with alias\n",
    "avg_by_name = df.groupBy(\"name\").agg(F.avg(\"age\").alias(\"average_age\"))\n",
    "avg_by_name.show()\n",
    "\n",
    "# Example 2: Multiple aggregations with aliases\n",
    "stats_by_name = df.groupBy(\"name\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"age\").alias(\"avg_age\"),\n",
    "    F.max(\"age\").alias(\"max_age\")\n",
    ")\n",
    "stats_by_name.show()\n",
    "\n",
    "# Example 3: Global aggregations (no groupBy) with aliases\n",
    "global_stats = df.agg(\n",
    "    F.count(\"*\").alias(\"total_people\"),\n",
    "    F.avg(\"age\").alias(\"overall_avg_age\")\n",
    ")\n",
    "global_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "665f5d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecc7c2",
   "metadata": {},
   "source": [
    "### Memory Management\n",
    "\n",
    "**Important Methods:**\n",
    "- `df.cache()` - Persists DataFrame in memory for reuse\n",
    "- `df.persist(storageLevel)` - Persists with specific storage level\n",
    "- `df.unpersist()` - Removes DataFrame from cache\n",
    "- Always unpersist when done to free memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d236067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "my_schema = T.StructType([\n",
    "    T.StructField(\"name\", T.StringType(), nullable=False),\n",
    "    T.StructField(\"age\", T.IntegerType(), nullable=False)\n",
    "])\n",
    "df_csv = spark.read.csv(\"people.csv\", schema=my_schema, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89655653",
   "metadata": {},
   "source": [
    "## 7. Schema Definition <a name=\"schema\"></a>\n",
    "\n",
    "### Explicit Schema for CSV Files\n",
    "\n",
    "Defining explicit schemas is recommended for better performance and data quality.\n",
    "\n",
    "**Benefits:**\n",
    "- Faster read performance (no schema inference needed)\n",
    "- Data type enforcement\n",
    "- Better error handling\n",
    "- Documentation of expected data structure\n",
    "\n",
    "**Common Data Types:**\n",
    "- `StringType()` - Text data\n",
    "- `IntegerType()`, `LongType()` - Integer numbers\n",
    "- `DoubleType()`, `FloatType()` - Decimal numbers\n",
    "- `BooleanType()` - True/False\n",
    "- `TimestampType()`, `DateType()` - Date/time\n",
    "- `ArrayType()`, `MapType()` - Complex types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9298e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 34|\n",
      "|    Bob| 45|\n",
      "|Charlie| 29|\n",
      "|  Diana| 28|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6fc30428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------------+\n",
      "|name   |age|concatenated          |\n",
      "+-------+---+----------------------+\n",
      "|Alice  |34 |Name: Alice, Age: 34  |\n",
      "|Bob    |45 |Name: Bob, Age: 45    |\n",
      "|Charlie|29 |Name: Charlie, Age: 29|\n",
      "|Diana  |28 |Name: Diana, Age: 28  |\n",
      "+-------+---+----------------------+\n",
      "\n",
      "+-------+---+---------------------+\n",
      "|name   |age|concat_with_separator|\n",
      "+-------+---+---------------------+\n",
      "|Alice  |34 |Alice - 34           |\n",
      "|Bob    |45 |Bob - 45             |\n",
      "|Charlie|29 |Charlie - 29         |\n",
      "|Diana  |28 |Diana - 28           |\n",
      "+-------+---+---------------------+\n",
      "\n",
      "+--------------------------------------+\n",
      "|description                           |\n",
      "+--------------------------------------+\n",
      "|Person | Alice | is | 34 | years old  |\n",
      "|Person | Bob | is | 45 | years old    |\n",
      "|Person | Charlie | is | 29 | years old|\n",
      "|Person | Diana | is | 28 | years old  |\n",
      "+--------------------------------------+\n",
      "\n",
      "+--------------------------------------+\n",
      "|description                           |\n",
      "+--------------------------------------+\n",
      "|Person | Alice | is | 34 | years old  |\n",
      "|Person | Bob | is | 45 | years old    |\n",
      "|Person | Charlie | is | 29 | years old|\n",
      "|Person | Diana | is | 28 | years old  |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, concat_ws, lit\n",
    "\n",
    "# Example 1: concat - concatenates columns without separator\n",
    "df_concat = df_csv.select(\n",
    "    \"name\",\n",
    "    \"age\",\n",
    "    concat(lit(\"Name: \"), F.col(\"name\"), lit(\", Age: \"), F.col(\"age\").cast(\"string\")).alias(\"concatenated\")\n",
    ")\n",
    "df_concat.show(truncate=False)\n",
    "\n",
    "# Example 2: concat_ws - concatenates columns with separator\n",
    "df_concat_ws = df_csv.select(\n",
    "    \"name\",\n",
    "    \"age\",\n",
    "    concat_ws(\" - \", F.col(\"name\"), F.col(\"age\").cast(\"string\")).alias(\"concat_with_separator\")\n",
    ")\n",
    "df_concat_ws.show(truncate=False)\n",
    "\n",
    "# Example 3: concat_ws with multiple columns and custom separator\n",
    "df_concat_multi = df_csv.select(\n",
    "    concat_ws(\" | \", lit(\"Person\"), F.col(\"name\"), lit(\"is\"), F.col(\"age\").cast(\"string\"), lit(\"years old\")).alias(\"description\")\n",
    ")\n",
    "df_concat_multi.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f64b49",
   "metadata": {},
   "source": [
    "## 8. String Operations <a name=\"string-operations\"></a>\n",
    "\n",
    "### String Concatenation Functions\n",
    "\n",
    "PySpark provides powerful string manipulation functions.\n",
    "\n",
    "**concat() vs concat_ws():**\n",
    "- `concat()` - Concatenates columns/literals without separator\n",
    "- `concat_ws(sep, col1, col2, ...)` - Concatenates with separator\n",
    "- `lit()` - Creates literal/constant value\n",
    "\n",
    "**Important Notes:**\n",
    "- Non-string columns must be cast to string: `.cast(\"string\")`\n",
    "- `concat_ws()` ignores null values\n",
    "- Use `lit()` to add constant text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a1c44",
   "metadata": {},
   "source": [
    "## 9. Additional Resources and Best Practices\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always use explicit schemas** when reading data in production\n",
    "2. **Use `F.col()`** instead of string column references for complex operations\n",
    "3. **Avoid `collect()`** on large datasets - use `take()` or `show()` instead\n",
    "4. **Cache DataFrames** that are used multiple times\n",
    "5. **Use column aliases** for better readability\n",
    "6. **Monitor partition count** for optimal performance\n",
    "7. **Set appropriate log levels** to reduce noise\n",
    "\n",
    "### Common DataFrame Methods Reference\n",
    "\n",
    "**Display & Inspection:**\n",
    "- `df.show(n, truncate)` - Display data\n",
    "- `df.printSchema()` - Show schema\n",
    "- `df.columns` - List column names\n",
    "- `df.dtypes` - Column names and types\n",
    "- `df.describe()` - Statistical summary\n",
    "- `df.count()` - Row count\n",
    "\n",
    "**Transformations:**\n",
    "- `df.select()` - Select columns\n",
    "- `df.filter()` / `df.where()` - Filter rows\n",
    "- `df.withColumn(name, column)` - Add/modify column\n",
    "- `df.withColumnRenamed(old, new)` - Rename column\n",
    "- `df.drop(column)` - Remove column\n",
    "- `df.distinct()` - Remove duplicates\n",
    "- `df.orderBy()` / `df.sort()` - Sort data\n",
    "\n",
    "**Aggregations:**\n",
    "- `df.groupBy().agg()` - Group and aggregate\n",
    "- `df.agg()` - Global aggregation\n",
    "- `df.groupBy().count()` - Count by group\n",
    "\n",
    "**Joins:**\n",
    "- `df1.join(df2, on, how)` - Join DataFrames\n",
    "- Join types: \"inner\", \"outer\", \"left\", \"right\", \"cross\"\n",
    "\n",
    "**I/O Operations:**\n",
    "- `spark.read.csv()`, `.json()`, `.parquet()` - Read files\n",
    "- `df.write.csv()`, `.json()`, `.parquet()` - Write files\n",
    "- Options: `.mode(\"overwrite\")`, `.mode(\"append\")`\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Partitioning:** Use `repartition()` or `coalesce()` to optimize partition count\n",
    "- **Broadcasting:** Use `F.broadcast()` for small DataFrames in joins\n",
    "- **Predicate Pushdown:** Filter early in your transformation chain\n",
    "- **Column Pruning:** Select only needed columns early\n",
    "- **Caching:** Cache intermediate results used multiple times\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Spark Learning! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scheduler_agent (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
