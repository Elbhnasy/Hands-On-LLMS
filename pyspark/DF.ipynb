{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed018f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/19 14:30:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/19 14:30:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde2e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|    name|\n",
      "+---+--------+\n",
      "| 30| John D.|\n",
      "| 25|Alice G.|\n",
      "| 35|  Bob T.|\n",
      "| 28|  Eve A.|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = [{\"name\": \"John D.\", \"age\": 30},\n",
    "  {\"name\": \"Alice G.\", \"age\": 25},\n",
    "  {\"name\": \"Bob T.\", \"age\": 35},\n",
    "  {\"name\": \"Eve A.\", \"age\": 28}]\n",
    "\n",
    "# Create a DataFrame containing the employees data\n",
    "df = spark.createDataFrame(employees)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe0560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John D.', 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = df.collect()[0]\n",
    "r1.name, r1.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c313e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================================>                   (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    name|avg(age)|\n",
      "+--------+--------+\n",
      "| John D.|    30.0|\n",
      "|Alice G.|    25.0|\n",
      "|  Bob T.|    35.0|\n",
      "|  Eve A.|    28.0|\n",
      "+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_age = df.groupBy('name').avg('age')\n",
    "avg_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a20a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22cb9eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_age.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e40612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- average_age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_age = df.groupBy('name').agg(avg('age').alias('average_age'))\n",
    "\n",
    "avg_age.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d8c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    name|average_age|\n",
      "+--------+-----------+\n",
      "| John D.|       30.0|\n",
      "|Alice G.|       25.0|\n",
      "|  Bob T.|       35.0|\n",
      "|  Eve A.|       28.0|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ed4fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"people.json\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b705417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|age |name   |\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|30  |Andy   |\n",
      "|19  |Justin |\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe95df35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57fe0880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.select(F.col(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "862c7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_names = df.select(\"name\")\n",
    "df_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c960b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|NULL|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_age = df.select(\"age\")\n",
    "df_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9b256e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 30|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_adults= df_age.where(F.col(\"age\") >= 18)\n",
    "df_adults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a4c32d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   name|average_age|\n",
      "+-------+-----------+\n",
      "|Michael|       NULL|\n",
      "|   Andy|       30.0|\n",
      "| Justin|       19.0|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+-----+-------+-------+\n",
      "|   name|count|avg_age|max_age|\n",
      "+-------+-----+-------+-------+\n",
      "|Michael|    1|   NULL|   NULL|\n",
      "|   Andy|    1|   30.0|     30|\n",
      "| Justin|    1|   19.0|     19|\n",
      "+-------+-----+-------+-------+\n",
      "\n",
      "+------------+---------------+\n",
      "|total_people|overall_avg_age|\n",
      "+------------+---------------+\n",
      "|           3|           24.5|\n",
      "+------------+---------------+\n",
      "\n",
      "+------------+---------------+\n",
      "|total_people|overall_avg_age|\n",
      "+------------+---------------+\n",
      "|           3|           24.5|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_by_name = df.groupBy(\"name\").agg(F.avg(\"age\").alias(\"average_age\"))\n",
    "avg_by_name.show()\n",
    "\n",
    "# Example 2: multiple aggregations with aliases\n",
    "stats_by_name = df.groupBy(\"name\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"age\").alias(\"avg_age\"),\n",
    "    F.max(\"age\").alias(\"max_age\")\n",
    ")\n",
    "stats_by_name.show()\n",
    "\n",
    "# Example 3: global aggregations (no groupBy) with aliases\n",
    "global_stats = df.agg(\n",
    "    F.count(\"*\").alias(\"total_people\"),\n",
    "    F.avg(\"age\").alias(\"overall_avg_age\")\n",
    ")\n",
    "global_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "665f5d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d236067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "my_schema = T.StructType([\n",
    "    T.StructField(\"name\", T.StringType(), nullable=False),\n",
    "    T.StructField(\"age\", T.IntegerType(), nullable=False)\n",
    "])\n",
    "df_csv = spark.read.csv(\"people.csv\", schema=my_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9298e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 34|\n",
      "|    Bob| 45|\n",
      "|Charlie| 29|\n",
      "|  Diana| 28|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc844670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scheduler_agent (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
