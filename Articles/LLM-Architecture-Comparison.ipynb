{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Big LLM Architecture Comparison - Study Guide\n",
    "\n",
    "**Based on Sebastian Raschka's Article (2025)**\n",
    "\n",
    "This notebook covers the architectural developments in modern Large Language Models (LLMs) from 2024-2025, focusing on structural changes rather than training techniques or benchmarks.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction & Context](#introduction)\n",
    "2. [DeepSeek V3/R1](#deepseek)\n",
    "   - Multi-Head Latent Attention (MLA)\n",
    "   - Mixture-of-Experts (MoE)\n",
    "3. [OLMo 2](#olmo)\n",
    "   - Normalization Layer Placement\n",
    "   - QK-Norm\n",
    "4. [Gemma 3](#gemma)\n",
    "   - Sliding Window Attention\n",
    "   - Normalization Strategies\n",
    "5. [Mistral Small 3.1](#mistral)\n",
    "6. [Llama 4](#llama)\n",
    "7. [Qwen3](#qwen)\n",
    "8. [SmolLM3](#smollm)\n",
    "   - NoPE (No Positional Embeddings)\n",
    "9. [Kimi 2](#kimi)\n",
    "10. [GPT-OSS](#gptoss)\n",
    "11. [Grok 2.5](#grok)\n",
    "12. [GLM-4.5](#glm)\n",
    "13. [Qwen3-Next](#qwen-next)\n",
    "14. [Key Takeaways](#takeaways)\n",
    "15. [Architecture Comparison Table](#comparison)\n",
    "16. [Practical Implications](#practical)\n",
    "17. [Code Examples](#code)\n",
    "18. [Further Reading & Resources](#resources)\n",
    "19. [Study Tips](#tips)\n",
    "20. [Practice Exercises](#exercises)\n",
    "21. [Quick Quiz](#quiz)\n",
    "22. [Revision Checklist](#checklist)\n",
    "23. [Additional Resources & Next Steps](#nextsteps)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1. Introduction & Context\n",
    "\n",
    "### Historical Perspective\n",
    "It has been **7 years** since the original GPT architecture. While models from GPT-2 (2019) to DeepSeek-V3 and Llama 4 (2024-2025) appear structurally similar, several key refinements have emerged:\n",
    "\n",
    "**Evolution of Components:**\n",
    "- **Positional Embeddings**: Absolute → Rotational (RoPE)\n",
    "- **Attention Mechanism**: Multi-Head Attention (MHA) → Grouped-Query Attention (GQA)\n",
    "- **Activation Functions**: GELU → SwiGLU\n",
    "\n",
    "### Why This Comparison Matters\n",
    "While benchmarks vary due to different:\n",
    "- Datasets\n",
    "- Training techniques  \n",
    "- Hyperparameters\n",
    "\n",
    "Examining architectural choices helps us understand what LLM developers prioritize in 2025.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deepseek'></a>\n",
    "## 2. DeepSeek V3/R1\n",
    "\n",
    "**Release**: December 2024 (V3), January 2025 (R1)\n",
    "\n",
    "**Key Stats:**\n",
    "- Total Parameters: 671B\n",
    "- Active Parameters: 37B (only 5.5% used during inference)\n",
    "- Major Impact: DeepSeek R1 showed strong reasoning capabilities\n",
    "\n",
    "### 2.1 Multi-Head Latent Attention (MLA)\n",
    "\n",
    "#### Background: Grouped-Query Attention (GQA)\n",
    "Before MLA, GQA became the standard for efficiency:\n",
    "\n",
    "**Traditional Multi-Head Attention (MHA)**:\n",
    "- Each head has its own Q, K, V projections\n",
    "- High memory usage for KV cache\n",
    "\n",
    "**Grouped-Query Attention (GQA)**:\n",
    "- Multiple query heads share the same K, V projections\n",
    "- Example: 4 attention heads with 2 KV groups\n",
    "  - Heads 1 & 2 share one K,V pair\n",
    "  - Heads 3 & 4 share another K,V pair\n",
    "- **Benefits**: Lower memory, fewer parameters\n",
    "- **Performance**: Comparable to MHA (per ablation studies)\n",
    "\n",
    "#### Multi-Head Latent Attention (MLA)\n",
    "**Different Strategy:**\n",
    "- Instead of sharing KV heads, MLA **compresses** K and V tensors\n",
    "- Compressed tensors stored in KV cache (lower dimensional space)\n",
    "- At inference: compressed tensors projected back to original size\n",
    "- Adds extra matrix multiplication but saves significant memory\n",
    "\n",
    "**Why MLA over GQA?**\n",
    "According to DeepSeek-V2 ablation studies:\n",
    "- GQA performs worse than MHA\n",
    "- **MLA performs BETTER than MHA** (slight improvement)\n",
    "- This is why DeepSeek chose MLA\n",
    "\n",
    "```\n",
    "Comparison:\n",
    "┌─────────────┬──────────────┬─────────────────┐\n",
    "│ Method      │ Performance  │ Memory Savings  │\n",
    "├─────────────┼──────────────┼─────────────────┤\n",
    "│ MHA         │ Baseline     │ No savings      │\n",
    "│ GQA         │ Slightly ↓   │ Good            │\n",
    "│ MLA         │ Slightly ↑   │ Very Good       │\n",
    "└─────────────┴──────────────┴─────────────────┘\n",
    "```\n",
    "\n",
    "### 2.2 Mixture-of-Experts (MoE)\n",
    "\n",
    "**Core Concept:**\n",
    "Replace single FeedForward block with multiple expert FeedForward blocks\n",
    "\n",
    "**How it Works:**\n",
    "1. Each transformer block has multiple expert layers (FeedForward modules)\n",
    "2. Router selects only a subset of experts per token\n",
    "3. Most parameters remain inactive during inference\n",
    "\n",
    "**DeepSeek V3 Configuration:**\n",
    "- 256 experts per MoE module\n",
    "- Only 9 active at a time (1 shared + 8 routed)\n",
    "- 671B total parameters → 37B active parameters\n",
    "\n",
    "**Terminology:**\n",
    "- **Sparse**: Only subset of parameters used (MoE)\n",
    "- **Dense**: All parameters always used (traditional)\n",
    "\n",
    "#### Shared Expert Innovation\n",
    "**What**: One expert that is ALWAYS active for every token\n",
    "\n",
    "**Why it Helps:**\n",
    "- Common/repeated patterns don't need to be learned by multiple experts\n",
    "- Individual experts can specialize in unique patterns\n",
    "- Boosts overall performance (per DeepSpeedMoE paper)\n",
    "\n",
    "**Key Insight:**\n",
    "> MoE allows massive parameter counts (knowledge capacity) while keeping inference efficient through sparsity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='olmo'></a>\n",
    "## 3. OLMo 2\n",
    "\n",
    "**Release**: January 2025\n",
    "**Organization**: Allen Institute for AI (non-profit)\n",
    "\n",
    "**Notable Features:**\n",
    "- Full transparency (training data, code, technical reports)\n",
    "- Great blueprint for developing LLMs\n",
    "- At release, sat at Pareto frontier of compute-to-performance\n",
    "\n",
    "### 3.1 Normalization Layer Placement\n",
    "\n",
    "**Historical Context:**\n",
    "\n",
    "1. **Original Transformer (2017) - Post-LN**:\n",
    "   - Normalization AFTER attention and FeedForward\n",
    "   - Outside residual connections\n",
    "\n",
    "2. **GPT-2 and Most Modern LLMs - Pre-LN**:\n",
    "   - Normalization BEFORE attention and FeedForward\n",
    "   - Benefits (2020 study):\n",
    "     - Better gradient behavior at initialization\n",
    "     - Works without careful learning rate warm-up\n",
    "\n",
    "3. **OLMo 2 - Modified Post-Norm**:\n",
    "   - Normalization AFTER modules BUT\n",
    "   - Still INSIDE residual connections (key difference!)\n",
    "\n",
    "**Why the Change?**\n",
    "- Improved training stability\n",
    "- Combined with QK-Norm (see next section)\n",
    "\n",
    "```\n",
    "Architecture Flow:\n",
    "\n",
    "Pre-Norm (GPT-2, Llama 3):          OLMo 2 Post-Norm:\n",
    "Input                               Input\n",
    "  ↓                                   ↓\n",
    "[Norm]                              [Attention]\n",
    "  ↓                                   ↓\n",
    "[Attention] ──→ Add                 [Norm] ──→ Add\n",
    "  ↓                                   ↓\n",
    "[Norm]                              [FeedForward]\n",
    "  ↓                                   ↓\n",
    "[FeedForward] ──→ Add               [Norm] ──→ Add\n",
    "```\n",
    "\n",
    "### 3.2 QK-Norm\n",
    "\n",
    "**What**: Additional RMSNorm layers inside the attention mechanism\n",
    "\n",
    "**Where Applied:**\n",
    "- To Queries (Q) before RoPE\n",
    "- To Keys (K) before RoPE\n",
    "\n",
    "**Implementation Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK-Norm applied in attention layer\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Normalization\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # RMS normalization\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        x_normed = x / rms\n",
    "        return self.weight * x_normed\n",
    "\n",
    "# Simplified QK-Norm in attention\n",
    "class QKNormAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qk_norm=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.qk_norm = qk_norm\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(self.head_dim)\n",
    "            self.k_norm = RMSNorm(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply QK-Norm if enabled\n",
    "        if self.qk_norm:\n",
    "            q = self.q_norm(q)\n",
    "            k = self.k_norm(k)\n",
    "        \n",
    "        # Attention computation (simplified, no RoPE here)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.o_proj(out)\n",
    "\n",
    "# Example\n",
    "d_model, n_heads = 512, 8\n",
    "attn = QKNormAttention(d_model, n_heads, qk_norm=True)\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = attn(x)\n",
    "print(\"QK-Norm applied in attention layer\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits:**\n",
    "- Stabilizes training loss (shown in OLMo 2 paper)\n",
    "- Used by multiple models (OLMo 2, Gemma 2, Gemma 3)\n",
    "\n",
    "**Architecture Comparison:**\n",
    "```\n",
    "OLMo 2 vs Llama 3:\n",
    "- Both use RMSNorm, RoPE, SwiGLU\n",
    "- OLMo 2: Post-Norm + QK-Norm + MHA\n",
    "- Llama 3: Pre-Norm + No QK-Norm + GQA\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gemma'></a>\n",
    "## 4. Gemma 3\n",
    "\n",
    "**Release**: 2025\n",
    "**Developer**: Google\n",
    "\n",
    "**Notable Aspects:**\n",
    "- Large vocabulary size (supports multiple languages)\n",
    "- Focus on 27B size (sweet spot: more capable than 8B, less resource-intensive than 70B)\n",
    "- Also available: 1B, 4B, 12B variants\n",
    "- Runs efficiently on local hardware (e.g., Mac Mini)\n",
    "\n",
    "### 4.1 Sliding Window Attention\n",
    "\n",
    "**Purpose**: Reduce KV cache memory requirements\n",
    "\n",
    "**Concept:**\n",
    "- **Global Attention (Regular)**: Each token can attend to ALL other tokens\n",
    "- **Local Attention (Sliding Window)**: Each token attends only to a limited window of nearby tokens\n",
    "\n",
    "**Implementation Details:**\n",
    "\n",
    "```\n",
    "Regular Attention:              Sliding Window Attention:\n",
    "Token at position t             Token at position t\n",
    "can attend to:                  can attend to:\n",
    "All positions [0...n]           Positions [t-window...t]\n",
    "                                (Local window around t)\n",
    "```\n",
    "\n",
    "**Gemma 2 vs Gemma 3 Configuration:**\n",
    "\n",
    "| Aspect | Gemma 2 | Gemma 3 |\n",
    "|--------|---------|----------|\n",
    "| Global:Local Ratio | 1:1 | 1:5 |\n",
    "| Sliding Window Size | 4096 tokens | 1024 tokens |\n",
    "| Strategy | Every other layer | Only 1 full attention per 5 sliding |\n",
    "\n",
    "**Memory Savings:**\n",
    "- Substantial KV cache reduction shown in Gemma 3 paper\n",
    "- Focus shifted more toward efficient, localized computations\n",
    "\n",
    "**Performance Impact:**\n",
    "- Ablation studies show **minimal impact** on perplexity\n",
    "- Trade-off: Slightly reduced context access for much better efficiency\n",
    "\n",
    "**Compatibility:**\n",
    "- Works with both MHA and GQA\n",
    "- Gemma 3 uses GQA\n",
    "\n",
    "### 4.2 Normalization Layer Placement\n",
    "\n",
    "**Unique Approach**: Pre-Norm AND Post-Norm simultaneously\n",
    "\n",
    "```\n",
    "Gemma 3 Architecture:\n",
    "\n",
    "Input\n",
    "  ↓\n",
    "[RMSNorm] ← Pre-Norm\n",
    "  ↓\n",
    "[Grouped-Query Attention]\n",
    "  ↓\n",
    "[RMSNorm] ← Post-Norm\n",
    "  ↓\n",
    "Add (residual)\n",
    "  ↓\n",
    "[RMSNorm] ← Pre-Norm\n",
    "  ↓\n",
    "[FeedForward]\n",
    "  ↓\n",
    "[RMSNorm] ← Post-Norm\n",
    "  ↓\n",
    "Add (residual)\n",
    "```\n",
    "\n",
    "**Rationale:**\n",
    "- Gets \"best of both worlds\"\n",
    "- Extra normalization rarely hurts\n",
    "- RMSNorm is computationally cheap\n",
    "- If redundant, minimal efficiency impact\n",
    "\n",
    "### 4.3 Gemma 3n Bonus\n",
    "\n",
    "**Released**: Few months after Gemma 3\n",
    "**Goal**: Optimized for small devices (phones)\n",
    "\n",
    "#### Per-Layer Embedding (PLE)\n",
    "**Strategy:**\n",
    "- Keep only subset of parameters in GPU memory\n",
    "- Stream token-specific embeddings from CPU/SSD on demand\n",
    "- Significant memory savings\n",
    "\n",
    "#### MatFormer (Matryoshka Transformer)\n",
    "**Concept:**\n",
    "- Single shared architecture that can be \"sliced\"\n",
    "- Each slice is independently usable\n",
    "- Run only the part you need at inference\n",
    "- Like Russian nesting dolls for transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mistral'></a>\n",
    "## 5. Mistral Small 3.1\n",
    "\n",
    "**Release**: March 2025 (shortly after Gemma 3)\n",
    "**Size**: 24B parameters\n",
    "\n",
    "**Key Achievement:**\n",
    "- Outperforms Gemma 3 27B on most benchmarks (except math)\n",
    "- Faster inference latency\n",
    "\n",
    "### Architecture Insights\n",
    "\n",
    "**Efficiency Sources:**\n",
    "1. Custom tokenizer\n",
    "2. Reduced KV cache size\n",
    "3. Fewer layers\n",
    "\n",
    "**Notable Change:**\n",
    "- Earlier Mistral models used sliding window attention\n",
    "- Mistral Small 3.1 **abandoned it**\n",
    "- Returned to regular GQA\n",
    "\n",
    "**Why Drop Sliding Window?**\n",
    "\n",
    "Speculation:\n",
    "- Regular GQA allows more optimized code (FlashAttention)\n",
    "- Sliding window reduces memory but may not reduce latency\n",
    "- Focus shifted to inference speed over memory\n",
    "\n",
    "```\n",
    "Mistral Small 3.1 Architecture:\n",
    "- Standard transformer with RMSNorm\n",
    "- Grouped-Query Attention (no sliding window)\n",
    "- RoPE positional embeddings\n",
    "- SwiGLU activation\n",
    "- Optimized for speed\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='llama'></a>\n",
    "## 6. Llama 4\n",
    "\n",
    "**Release**: 2024-2025\n",
    "**Developer**: Meta\n",
    "\n",
    "**Key Development**: Adopted MoE architecture (joining the trend)\n",
    "\n",
    "### Llama 4 Maverick (400B total, 17B active)\n",
    "\n",
    "**Architecture Comparison with DeepSeek-V3:**\n",
    "\n",
    "| Feature | Llama 4 Maverick | DeepSeek-V3 |\n",
    "|---------|------------------|-------------|\n",
    "| Total Parameters | 400B | 671B |\n",
    "| Active Parameters | 17B | 37B |\n",
    "| Attention Type | GQA | MLA |\n",
    "| Active Experts | 2 | 9 (8 + 1 shared) |\n",
    "| Expert Hidden Size | 8,192 | 2,048 |\n",
    "| MoE Placement | Every other block | Every block (except first 3) |\n",
    "\n",
    "### Design Philosophy Differences\n",
    "\n",
    "**Llama 4 Approach:**\n",
    "- Fewer, larger experts\n",
    "- Classic MoE setup\n",
    "- Alternates MoE and dense layers\n",
    "\n",
    "**DeepSeek V3 Approach:**\n",
    "- Many small experts\n",
    "- Shared expert included\n",
    "- MoE in almost all layers\n",
    "\n",
    "**Similarities:**\n",
    "- Both use RMSNorm, RoPE, SwiGLU\n",
    "- Both adopt MoE for efficiency\n",
    "- Similar overall transformer structure\n",
    "\n",
    "**Key Insight:**\n",
    "> MoE architectures saw significant rise in popularity in 2025, with different implementation strategies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qwen'></a>\n",
    "## 7. Qwen3\n",
    "\n",
    "**Developer**: Qwen Team (Alibaba)\n",
    "**Release**: 2025\n",
    "\n",
    "**Notable Achievement:**\n",
    "- Consistently tops leaderboards for their size classes\n",
    "- Won NeurIPS 2023 LLM efficiency challenge (Qwen2)\n",
    "- High-quality open-weight models\n",
    "\n",
    "### Model Variants\n",
    "\n",
    "**Dense Models:**\n",
    "- 0.6B, 1.7B, 4B, 8B, 14B, 32B\n",
    "\n",
    "**MoE Models:**\n",
    "- 30B-A3B (30B total, 3B active)\n",
    "- 235B-A22B (235B total, 22B active)\n",
    "\n",
    "### 7.1 Qwen3 Dense (Focus on 0.6B)\n",
    "\n",
    "**Why 0.6B is Notable:**\n",
    "- Smallest current-generation open-weight model\n",
    "- Excellent performance for size\n",
    "- Great token/sec throughput\n",
    "- Low memory footprint\n",
    "- Easy to train locally (educational purposes)\n",
    "\n",
    "**Architectural Strategy:**\n",
    "\n",
    "```\n",
    "Qwen3 0.6B:           Llama 3 1B:\n",
    "- Deeper (more layers)  - Wider (more heads)\n",
    "- Fewer attention heads - More attention heads\n",
    "- Smaller hidden dim    - Larger hidden dim\n",
    "- More transformer blocks - Fewer transformer blocks\n",
    "```\n",
    "\n",
    "**Trade-off:**\n",
    "- Qwen3: Smaller memory, slower tokens/sec (more sequential operations)\n",
    "- Llama 3: Larger memory, faster tokens/sec (more parallelization)\n",
    "\n",
    "### 7.2 Qwen3 MoE\n",
    "\n",
    "**Why Both Dense and MoE?**\n",
    "\n",
    "**Dense Models:**\n",
    "- Simpler to fine-tune\n",
    "- Easier to deploy\n",
    "- Better hardware compatibility\n",
    "- More robust\n",
    "\n",
    "**MoE Models:**\n",
    "- Higher capacity (more knowledge)\n",
    "- Lower inference cost for given capacity\n",
    "- Efficient scaling\n",
    "\n",
    "**Qwen3 235B-A22B Architecture:**\n",
    "- 256 experts\n",
    "- 8 active per token\n",
    "- **NO shared expert** (changed from Qwen2.5-MoE)\n",
    "- Uses GQA\n",
    "\n",
    "**Why Remove Shared Expert?**\n",
    "\n",
    "Official response from Junyang Lin (Qwen3 developer):\n",
    "> \"At that moment we did not find significant enough improvement on shared expert and we were worrying about the optimization for inference caused by shared expert.\"\n",
    "\n",
    "**Speculation:**\n",
    "- With more experts (8 vs 2), shared expert may become less necessary\n",
    "- Saves compute/memory (8 experts vs 8+1)\n",
    "- Training stability already achieved\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='smollm'></a>\n",
    "## 8. SmolLM3\n",
    "\n",
    "**Size**: 3B parameters\n",
    "**Developer**: Hugging Face\n",
    "\n",
    "**Key Features:**\n",
    "- Sweet spot size (between 1.7B and 4B Qwen3)\n",
    "- Great performance for size\n",
    "- Shared training details (transparency)\n",
    "- Outperforms some larger models\n",
    "\n",
    "### 8.1 NoPE (No Positional Embeddings)\n",
    "\n",
    "**Background: Why Positional Embeddings?**\n",
    "\n",
    "Self-attention is **permutation invariant**:\n",
    "- Treats \"cat sat on mat\" same as \"mat on sat cat\"\n",
    "- Needs position information\n",
    "\n",
    "**Traditional Solutions:**\n",
    "\n",
    "1. **Absolute Positional Embeddings**:\n",
    "   - Add learned position vectors to token embeddings\n",
    "   - Example: pos_embedding[0] + token_embedding[\"cat\"]\n",
    "\n",
    "2. **RoPE (Rotary Position Embedding)**:\n",
    "   - Rotate Q and K vectors based on position\n",
    "   - Used by most modern LLMs\n",
    "\n",
    "**NoPE Approach:**\n",
    "- **No positional information added at all**\n",
    "- No fixed embeddings\n",
    "- No learned embeddings\n",
    "- No relative encoding\n",
    "- Literally nothing\n",
    "\n",
    "**How Does It Work?**\n",
    "\n",
    "**Key Insight**: Causal attention mask provides implicit ordering!\n",
    "\n",
    "```\n",
    "Causal Mask:\n",
    "Token at position t can only see positions ≤ t\n",
    "\n",
    "Example:\n",
    "\"The cat sat\"\n",
    "- \"The\" sees: [The]\n",
    "- \"cat\" sees: [The, cat]\n",
    "- \"sat\" sees: [The, cat, sat]\n",
    "\n",
    "This creates implicit directional flow!\n",
    "```\n",
    "\n",
    "**Benefits of NoPE:**\n",
    "\n",
    "1. **Better Length Generalization**:\n",
    "   - Performance degrades less with longer sequences\n",
    "   - Not limited by learned position embeddings\n",
    "\n",
    "2. **Simplicity**:\n",
    "   - One less component to tune\n",
    "   - Fewer parameters\n",
    "\n",
    "**SmolLM3 Implementation:**\n",
    "- NoPE applied in **every 4th layer**\n",
    "- Other layers still use RoPE\n",
    "- Hybrid approach for safety\n",
    "\n",
    "**Caution:**\n",
    "- Original NoPE experiments: ~100M parameters\n",
    "- Unclear how well it scales to larger models\n",
    "- Hence the partial adoption in SmolLM3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kimi'></a>\n",
    "## 9. Kimi 2\n",
    "\n",
    "**Release**: 2025\n",
    "**Size**: 1 Trillion parameters (1T)\n",
    "**Developer**: Moonshot AI\n",
    "\n",
    "**Major Achievement:**\n",
    "- Performance on par with best proprietary models (GPT, Claude, Gemini)\n",
    "- May be largest LLM of this generation\n",
    "- Open-weight release\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "**1. Training Method:**\n",
    "- First production model to use **Muon optimizer** (over AdamW)\n",
    "- Previously only tested up to 16B parameters\n",
    "- Smooth training loss curves\n",
    "\n",
    "**2. Architecture:**\n",
    "- Based on DeepSeek-V3 architecture\n",
    "- Scaled up significantly\n",
    "\n",
    "### Kimi K2 vs DeepSeek-V3\n",
    "\n",
    "**Differences:**\n",
    "\n",
    "| Component | DeepSeek-V3 | Kimi K2 |\n",
    "|-----------|-------------|----------|\n",
    "| Total Parameters | 671B | 1000B (1T) |\n",
    "| MoE Experts | 256 | More (scaled up) |\n",
    "| MLA Heads | More | Fewer |\n",
    "| Layers | 61 | Scaled proportionally |\n",
    "\n",
    "**Otherwise:**\n",
    "- Same core architecture\n",
    "- Multi-Head Latent Attention (MLA)\n",
    "- Mixture-of-Experts with shared expert\n",
    "- RMSNorm, RoPE, SwiGLU\n",
    "\n",
    "### Historical Context\n",
    "\n",
    "**Kimi 1.5 (January 2025):**\n",
    "- Also impressive\n",
    "- Released same day as DeepSeek R1\n",
    "- Overshadowed by DeepSeek\n",
    "- Weights never publicly shared\n",
    "\n",
    "**Lesson Learned:**\n",
    "- Kimi K2 released BEFORE DeepSeek R2\n",
    "- Shared as open-weight immediately\n",
    "- Gained proper recognition\n",
    "\n",
    "**Current Status:**\n",
    "As of writing, **most impressive open-weight model available**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gptoss'></a>\n",
    "## 10. GPT-OSS\n",
    "\n",
    "**Release**: ~One week after article published\n",
    "**Developer**: OpenAI\n",
    "**Significance**: First open-weight OpenAI model since GPT-2 (2019)\n",
    "\n",
    "**Models:**\n",
    "- gpt-oss-20b (20B total, 3.6B active)\n",
    "- gpt-oss-120b (120B total)\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "**Standard Components:**\n",
    "- MoE architecture\n",
    "- Grouped-Query Attention\n",
    "- RMSNorm, RoPE, SwiGLU\n",
    "- Similar to other 2025 models\n",
    "\n",
    "**Unique Aspect:**\n",
    "- Sliding window attention (every other layer)\n",
    "- Like Gemma 3 but different ratio\n",
    "\n",
    "### 10.1 Width vs Depth\n",
    "\n",
    "**gpt-oss-20b vs Qwen3 30B-A3B:**\n",
    "\n",
    "Both similar active parameters (~3.6B vs ~3.3B)\n",
    "\n",
    "```\n",
    "gpt-oss:                    Qwen3:\n",
    "- Wider                     - Deeper\n",
    "- 24 layers                 - 48 layers\n",
    "- 2880 embedding dim        - 2048 embedding dim\n",
    "- 2880 expert dim           - 768 expert dim\n",
    "- More attention heads      - Fewer attention heads\n",
    "```\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "**Deeper Models:**\n",
    "- More flexibility\n",
    "- Harder to train (gradient issues)\n",
    "- Slower inference (sequential)\n",
    "\n",
    "**Wider Models:**\n",
    "- Faster inference (parallelization)\n",
    "- Higher memory cost\n",
    "- Better tokens/second\n",
    "\n",
    "**Evidence:**\n",
    "Gemma 2 ablation study (9B parameters):\n",
    "- Wider: 52.0 average score\n",
    "- Deeper: 50.8 average score\n",
    "- Slight advantage to width\n",
    "\n",
    "### 10.2 Few Large vs Many Small Experts\n",
    "\n",
    "**gpt-oss Configuration:**\n",
    "- 32 experts (surprisingly few)\n",
    "- 4 active per token\n",
    "- Each expert is large\n",
    "\n",
    "**Recent Trend:**\n",
    "- Move toward MORE, SMALLER experts\n",
    "- Better specialization\n",
    "- Improved performance\n",
    "\n",
    "**Example Evolution:**\n",
    "```\n",
    "DeepSeekMoE paper showed:\n",
    "16 experts → 32 experts → 64 experts\n",
    "(at constant total parameter count)\n",
    "= Better performance\n",
    "```\n",
    "\n",
    "gpt-oss goes against this trend (older design philosophy)\n",
    "\n",
    "### 10.3 Attention Bias Units\n",
    "\n",
    "**Surprising Detail:**\n",
    "- Uses bias units in attention layers\n",
    "- Not seen since GPT-2\n",
    "- Generally considered redundant\n",
    "\n",
    "**Research Evidence:**\n",
    "Recent paper showed:\n",
    "- Mathematically redundant (at least for K projection)\n",
    "- Little empirical difference in performance\n",
    "\n",
    "### 10.4 Attention Sinks\n",
    "\n",
    "**Purpose**: Stabilize attention in long contexts\n",
    "\n",
    "**Traditional Attention Sinks:**\n",
    "- Special tokens at sequence start\n",
    "- Always attended to\n",
    "- Store general sequence information\n",
    "\n",
    "**gpt-oss Implementation:**\n",
    "- Not actual tokens\n",
    "- Learned per-head bias logits\n",
    "- Appended to attention scores\n",
    "- Same goal, different method\n",
    "\n",
    "```python\n",
    "# Simplified attention sink implementation\n",
    "class AttentionWithSinks:\n",
    "    def __init__(self, num_heads, num_sinks=4):\n",
    "        # Learned sink logits (per head)\n",
    "        self.sinks = nn.Parameter(\n",
    "            torch.randn(num_heads, num_sinks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, attn_scores):\n",
    "        # Append sink logits to attention scores\n",
    "        attn_with_sinks = torch.cat([\n",
    "            self.sinks.unsqueeze(0).expand(batch, -1, -1),\n",
    "            attn_scores\n",
    "        ], dim=-1)\n",
    "        # Continue with softmax, etc.\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grok'></a>\n",
    "## 11. Grok 2.5\n",
    "\n",
    "**Release**: Weeks after article published\n",
    "**Developer**: xAI (Elon Musk)\n",
    "**Size**: 270B parameters\n",
    "\n",
    "**Significance:**\n",
    "- Real production model (not built for open-source)\n",
    "- xAI's flagship from last year\n",
    "- Rare look at actual deployed system\n",
    "\n",
    "### Architecture Characteristics\n",
    "\n",
    "**Overall**: Fairly standard, with notable choices\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Expert Configuration:**\n",
    "   - 8 experts (small number)\n",
    "   - Large experts\n",
    "   - Reflects older MoE trend\n",
    "\n",
    "2. **Shared Expert Design:**\n",
    "   - Additional SwiGLU module (always on)\n",
    "   - Functions as shared expert\n",
    "   - Intermediate dimension is doubled\n",
    "   - Not identical to classic shared expert but same idea\n",
    "\n",
    "3. **Standard Components:**\n",
    "   - GQA\n",
    "   - RMSNorm\n",
    "   - RoPE\n",
    "   - SwiGLU activation\n",
    "\n",
    "**Comparison with Modern Trends:**\n",
    "```\n",
    "Older Design (Grok 2.5):    Modern Design (DeepSeek V3):\n",
    "- Few large experts          - Many small experts\n",
    "- 8 experts                  - 256 experts\n",
    "- Production-tested          - Cutting-edge research\n",
    "```\n",
    "\n",
    "**Insight:**\n",
    "Shows that older MoE designs still work well in production, even if newer approaches may be theoretically better\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='glm'></a>\n",
    "## 12. GLM-4.5\n",
    "\n",
    "**Developer**: ZAI (Zhipu AI)\n",
    "**Release**: 2025\n",
    "\n",
    "**Key Achievement:**\n",
    "- Instruction/reasoning hybrid\n",
    "- Optimized for function calling and agents\n",
    "- Outperforms Claude 4 Opus on average (12 benchmarks)\n",
    "- Trails slightly behind OpenAI o3 and xAI Grok 4\n",
    "\n",
    "### Model Variants\n",
    "\n",
    "**1. GLM-4.5 (355B parameters)**:\n",
    "- Flagship model\n",
    "- Top-tier performance\n",
    "\n",
    "**2. GLM-4.5-Air (106B parameters)**:\n",
    "- Compact version\n",
    "- Performance only marginally below 355B\n",
    "- More efficient deployment\n",
    "\n",
    "### Architecture Innovation\n",
    "\n",
    "**Key Structural Choice: Dense Layers Before MoE**\n",
    "\n",
    "Adopted from DeepSeek V3:\n",
    "- **First 3 layers**: Dense (no MoE)\n",
    "- **Remaining layers**: MoE\n",
    "\n",
    "**Why Dense Layers First?**\n",
    "\n",
    "1. **Stability During Training:**\n",
    "   - MoE routing is inherently unstable early on\n",
    "   - Sparse expert selection can interfere with learning\n",
    "\n",
    "2. **Better Feature Extraction:**\n",
    "   - Low-level features need stable learning\n",
    "   - Syntactic and semantic basics formed first\n",
    "   - Dense layers ensure solid foundation\n",
    "\n",
    "3. **Improved Convergence:**\n",
    "   - More stable gradient flow initially\n",
    "   - MoE benefits appear after basics learned\n",
    "\n",
    "```\n",
    "GLM-4.5 Layer Structure:\n",
    "\n",
    "Layers 1-3:    [Dense FeedForward] ← Stable foundation\n",
    "Layers 4-n:    [MoE FeedForward]   ← Specialized processing\n",
    "```\n",
    "\n",
    "**Other Features:**\n",
    "- Uses shared expert (like DeepSeek V3)\n",
    "- Unlike Qwen3 which removed shared expert\n",
    "- Retains attention bias (like GPT-2, gpt-oss)\n",
    "- Standard: GQA, RMSNorm, RoPE, SwiGLU\n",
    "\n",
    "**Comparison:**\n",
    "```\n",
    "                    GLM-4.5    Qwen3      DeepSeek-V3\n",
    "Dense layers first:   ✓          ✗            ✓\n",
    "Shared expert:        ✓          ✗            ✓\n",
    "Attention bias:       ✓          ✗            ✗\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qwen-next'></a>\n",
    "## 13. Qwen3-Next\n",
    "\n",
    "**Release**: September 2025\n",
    "**Size**: 80B-A3B (80B total, 3B active)\n",
    "**Developer**: Qwen Team\n",
    "\n",
    "**Variants:**\n",
    "- Instruct\n",
    "- Thinking (reasoning-focused)\n",
    "\n",
    "### Major Architectural Changes\n",
    "\n",
    "### 13.1 Expert Configuration Evolution\n",
    "\n",
    "**Comparison: Qwen3 235B vs Qwen3-Next 80B**\n",
    "\n",
    "| Aspect | Qwen3 235B-A22B | Qwen3-Next 80B-A3B |\n",
    "|--------|-----------------|--------------------|\n",
    "| Total Size | 235B | 80B (3× smaller) |\n",
    "| Active Params | 22B | 3B |\n",
    "| Experts | 256 | **1024** (4× more) |\n",
    "| Shared Expert | No | **Yes** (added back) |\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Smaller model, MORE experts**:\n",
    "   - Goes against intuition\n",
    "   - Aligns with \"many small experts\" trend\n",
    "\n",
    "2. **Shared expert returned**:\n",
    "   - Removed in Qwen3\n",
    "   - Added back in Qwen3-Next\n",
    "   - Suggests benefits outweigh costs\n",
    "\n",
    "### 13.2 Gated DeltaNet + Gated Attention Hybrid\n",
    "\n",
    "**Purpose**: Enable 262k token context (massive increase from 32k)\n",
    "\n",
    "**Previous Limitation:**\n",
    "- Qwen3 235B: 32k native, 131k with YaRN scaling\n",
    "- Standard attention: memory grows with sequence length\n",
    "\n",
    "**New Hybrid Approach:**\n",
    "\n",
    "**1. Gated Attention (25% of layers)**:\n",
    "- Modified GQA with enhancements:\n",
    "  - Output gate (sigmoid-controlled, per-channel)\n",
    "  - Zero-centered RMSNorm for QK\n",
    "  - Partial RoPE (subset of dimensions)\n",
    "- Essentially stability improvements to GQA\n",
    "- Still scaled dot-product attention\n",
    "\n",
    "**2. Gated DeltaNet (75% of layers)**:\n",
    "- **Alternative to Mamba in \"linear-time\" family**\n",
    "- Fast-weight delta rule update\n",
    "- Produces q, k, v + two gates (α, β)\n",
    "- Linear and lightweight conv layers\n",
    "\n",
    "**Ratio: 3:1 (DeltaNet : Gated Attention)**\n",
    "\n",
    "```\n",
    "Layer Pattern:\n",
    "[DeltaNet] [DeltaNet] [DeltaNet] [Gated Attention]\n",
    "[DeltaNet] [DeltaNet] [DeltaNet] [Gated Attention]\n",
    "... (repeat)\n",
    "```\n",
    "\n",
    "**Trade-offs:**\n",
    "- DeltaNet: Less precise content retrieval (linear time)\n",
    "- Gated Attention: Precise but quadratic\n",
    "- Hybrid: Best of both (efficiency + accuracy)\n",
    "\n",
    "### 13.3 Multi-Token Prediction (MTP)\n",
    "\n",
    "**Concept:**\n",
    "- Traditional: Predict one token at a time\n",
    "- MTP: Predict multiple future tokens simultaneously\n",
    "\n",
    "**Implementation:**\n",
    "- Extra linear heads for tokens t+1, t+2, ..., t+k (k=4 recommended)\n",
    "- Sum cross-entropy losses for all offsets\n",
    "\n",
    "**Benefits:**\n",
    "1. **Training**: More signal, faster convergence\n",
    "2. **Inference**: Enables speculative decoding (accept/reject multiple tokens)\n",
    "3. **Qwen3-Next Specific**: Native MTP with high acceptance rate\n",
    "\n",
    "**Quote from Qwen Blog:**\n",
    "> \"Qwen3-Next introduces a native Multi-Token Prediction (MTP) mechanism, which not only yields an MTP module with a high acceptance rate for Speculative Decoding but also enhances the overall performance.\"\n",
    "\n",
    "**Summary:**\n",
    "- Smaller, more experts, shared expert back\n",
    "- DeltaNet hybrid for 262k context\n",
    "- MTP for faster training/inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='takeaways'></a>\n",
    "## 14. Key Takeaways & Trends\n",
    "\n",
    "### Major Architectural Trends in 2025\n",
    "\n",
    "#### 1. Rise of Mixture-of-Experts (MoE)\n",
    "\n",
    "**Widespread Adoption:**\n",
    "- DeepSeek V3/R1\n",
    "- Llama 4 Maverick\n",
    "- Qwen3 MoE variants\n",
    "- Kimi 2\n",
    "- gpt-oss\n",
    "- Grok 2.5\n",
    "- GLM-4.5\n",
    "\n",
    "**Evolution Pattern:**\n",
    "```\n",
    "Early MoE (2024):          Modern MoE (2025):\n",
    "- Few large experts        - Many small experts\n",
    "- 8-32 experts            - 64-256+ experts\n",
    "- Simple routing          - Shared experts\n",
    "- All layers MoE          - Strategic placement\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Massive parameter counts without proportional inference cost\n",
    "- Higher knowledge capacity\n",
    "- Efficient scaling\n",
    "\n",
    "#### 2. Attention Mechanism Evolution\n",
    "\n",
    "**Efficiency Strategies Compared:**\n",
    "\n",
    "| Method | Used By | Strategy | Benefits |\n",
    "|--------|---------|----------|----------|\n",
    "| **GQA** | Llama, Mistral, Qwen, most others | Share K,V across query heads | Standard efficiency |\n",
    "| **MLA** | DeepSeek, Kimi | Compress K,V tensors | Better performance + efficiency |\n",
    "| **Sliding Window** | Gemma 3 | Local attention windows | Massive KV cache savings |\n",
    "| **DeltaNet Hybrid** | Qwen3-Next | Linear-time alternative | Extreme context lengths |\n",
    "\n",
    "**Key Observation:**\n",
    "- Multiple solutions to same problem\n",
    "- No clear winner yet\n",
    "- Different trade-offs for different use cases\n",
    "\n",
    "#### 3. Normalization Strategies\n",
    "\n",
    "**Convergence to RMSNorm:**\n",
    "- Simpler than LayerNorm\n",
    "- Fewer parameters\n",
    "- Standard across all 2025 models\n",
    "\n",
    "**Placement Variations:**\n",
    "\n",
    "```\n",
    "Model            Placement Strategy\n",
    "──────────────────────────────────────────────\n",
    "GPT-2/Llama 3   Pre-Norm only\n",
    "OLMo 2          Post-Norm (modified)\n",
    "Gemma 3         Pre-Norm + Post-Norm both\n",
    "Most others     Pre-Norm (standard)\n",
    "```\n",
    "\n",
    "**QK-Norm Addition:**\n",
    "- Normalization inside attention (on Q and K)\n",
    "- Used by: OLMo 2, Gemma 2, Gemma 3\n",
    "- Improves training stability\n",
    "\n",
    "#### 4. Positional Encoding\n",
    "\n",
    "**Clear Winner: RoPE**\n",
    "- Rotary Position Embedding\n",
    "- Used by virtually all modern models\n",
    "- Replaces absolute position embeddings\n",
    "\n",
    "**Alternative Approach:**\n",
    "- SmolLM3: NoPE (No Position Embedding)\n",
    "- Still experimental\n",
    "- Good for length generalization\n",
    "- Partial adoption (every 4th layer)\n",
    "\n",
    "#### 5. Width vs Depth Trade-offs\n",
    "\n",
    "**Two Design Philosophies:**\n",
    "\n",
    "**Deeper Models (Qwen3):**\n",
    "- More layers\n",
    "- Smaller embedding dimension\n",
    "- More sequential processing\n",
    "- Pros: More flexible, smaller memory\n",
    "- Cons: Slower inference, harder to train\n",
    "\n",
    "**Wider Models (gpt-oss):**\n",
    "- Fewer layers\n",
    "- Larger embedding dimension\n",
    "- More parallel processing\n",
    "- Pros: Faster inference, better parallelization\n",
    "- Cons: Higher memory usage\n",
    "\n",
    "**Limited Evidence:**\n",
    "- Gemma 2 study: Slight advantage to width\n",
    "- More research needed\n",
    "\n",
    "#### 6. Expert Configuration Evolution\n",
    "\n",
    "**Trend: Many Small > Few Large**\n",
    "\n",
    "```\n",
    "Timeline:\n",
    "2024: 8-32 large experts → Basic specialization\n",
    "2025: 64-256+ small experts → Fine-grained specialization\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- DeepSeek V3: 256 experts\n",
    "- Qwen3: 256 experts\n",
    "- Qwen3-Next: 1024 experts (!) \n",
    "- vs older designs: 8-32 experts\n",
    "\n",
    "**Shared Expert Status:**\n",
    "- DeepSeek V3: Uses shared expert ✓\n",
    "- Kimi 2: Uses shared expert ✓\n",
    "- GLM-4.5: Uses shared expert ✓\n",
    "- Qwen3: Removed shared expert ✗\n",
    "- Qwen3-Next: Added back shared expert ✓\n",
    "\n",
    "#### 7. Dense Layers Before MoE\n",
    "\n",
    "**New Pattern (DeepSeek V3, GLM-4.5):**\n",
    "```\n",
    "Layers 1-3:    Dense FeedForward\n",
    "Layers 4+:     MoE FeedForward\n",
    "```\n",
    "\n",
    "**Reasoning:**\n",
    "- Stable foundation for low-level features\n",
    "- Better convergence\n",
    "- MoE benefits appear after basics learned\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "## 15. Architecture Comparison Table\n",
    "\n",
    "### Quick Reference: Model Characteristics\n",
    "\n",
    "| Model | Size | Attention | MoE | Norm Placement | Key Innovation |\n",
    "|-------|------|-----------|-----|----------------|----------------|\n",
    "| **DeepSeek V3** | 671B (37B active) | MLA | 256 experts | Pre-Norm | MLA + Shared expert |\n",
    "| **OLMo 2** | Various | MHA | No | Post-Norm | QK-Norm + transparency |\n",
    "| **Gemma 3** | 27B | GQA + Sliding | No | Pre+Post | 5:1 sliding window ratio |\n",
    "| **Mistral 3.1** | 24B | GQA | No | Pre-Norm | Speed optimization |\n",
    "| **Llama 4** | 400B (17B active) | GQA | Yes | Pre-Norm | MoE adoption |\n",
    "| **Qwen3** | 0.6B-235B | GQA | Optional | Pre-Norm | No shared expert (MoE) |\n",
    "| **SmolLM3** | 3B | GQA | No | Pre-Norm | NoPE (partial) |\n",
    "| **Kimi 2** | 1T | MLA | 256+ experts | Pre-Norm | Muon optimizer |\n",
    "| **gpt-oss** | 20B-120B | GQA + Sliding | Yes | Pre-Norm | Attention sinks + bias |\n",
    "| **Grok 2.5** | 270B | GQA | 8 experts | Pre-Norm | Production model |\n",
    "| **GLM-4.5** | 355B | GQA | Yes | Pre-Norm | Dense layers first |\n",
    "| **Qwen3-Next** | 80B (3B active) | DeltaNet hybrid | 1024 experts | Pre-Norm | DeltaNet + MTP |\n",
    "\n",
    "### Attention Strategy Distribution\n",
    "\n",
    "```\n",
    "GQA (Standard):          8 models (majority)\n",
    "MLA (DeepSeek):          2 models\n",
    "GQA + Sliding Window:    2 models\n",
    "DeltaNet Hybrid:         1 model (newest)\n",
    "Traditional MHA:         1 model (OLMo 2)\n",
    "```\n",
    "\n",
    "### MoE Adoption\n",
    "\n",
    "```\n",
    "Dense Only:              5 models (38%)\n",
    "MoE:                     8 models (62%)\n",
    "\n",
    "Trend: MoE becoming standard for large models\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical'></a>\n",
    "## 16. Practical Implications\n",
    "\n",
    "### For Researchers\n",
    "\n",
    "**Architectural Choices Matter:**\n",
    "- MLA shows performance gains over GQA\n",
    "- Many small experts > few large experts\n",
    "- Normalization placement affects stability\n",
    "- Dense layers before MoE improves convergence\n",
    "\n",
    "**Open Questions:**\n",
    "1. Why does MLA outperform GQA?\n",
    "2. Optimal expert count and size?\n",
    "3. Best normalization strategy?\n",
    "4. Width vs depth trade-offs?\n",
    "5. When to use sliding window vs full attention?\n",
    "\n",
    "### For Practitioners\n",
    "\n",
    "**Model Selection Guide:**\n",
    "\n",
    "**For Local Deployment:**\n",
    "- Small models: Qwen3 0.6B-4B, SmolLM3 3B\n",
    "- Medium models: Gemma 3 27B, Mistral Small 24B\n",
    "- Focus: Memory footprint, tokens/sec\n",
    "\n",
    "**For Production (Cloud):**\n",
    "- MoE models: Better cost/performance\n",
    "- DeepSeek V3, Llama 4, Qwen3 MoE\n",
    "- Focus: Active parameters, efficiency\n",
    "\n",
    "**For Fine-tuning:**\n",
    "- Dense models easier to fine-tune\n",
    "- Better hardware compatibility\n",
    "- OLMo 2, Gemma 3, Qwen3 dense\n",
    "\n",
    "**For Long Context:**\n",
    "- Sliding window: Gemma 3\n",
    "- DeltaNet hybrid: Qwen3-Next (262k tokens)\n",
    "- MLA: DeepSeek, Kimi\n",
    "\n",
    "**For Reasoning:**\n",
    "- DeepSeek R1\n",
    "- Kimi K2\n",
    "- Qwen3-Next (Thinking variant)\n",
    "\n",
    "### For Model Developers\n",
    "\n",
    "**Recommended Stack (2025):**\n",
    "```python\n",
    "# Modern LLM Architecture Template\n",
    "\n",
    "Architecture Components:\n",
    "├── Positional: RoPE (standard)\n",
    "├── Normalization: RMSNorm\n",
    "│   ├── Placement: Pre-Norm (safe)\n",
    "│   └── Optional: QK-Norm for stability\n",
    "├── Activation: SwiGLU\n",
    "├── Attention: GQA or MLA\n",
    "│   ├── GQA: Standard choice\n",
    "│   ├── MLA: Better performance, harder to implement\n",
    "│   └── Optional: Sliding window for efficiency\n",
    "└── FeedForward:\n",
    "    ├── Dense: Layers 1-3\n",
    "    └── MoE: Layers 4+\n",
    "        ├── Many small experts (64-256+)\n",
    "        ├── Include shared expert\n",
    "        └── Smart routing\n",
    "```\n",
    "\n",
    "**Implementation Priority:**\n",
    "1. Start with proven components (RoPE, RMSNorm, GQA)\n",
    "2. Add MoE for large models (>10B)\n",
    "3. Consider MLA for flagship models\n",
    "4. Experiment with normalization for stability\n",
    "5. Profile and optimize for your hardware\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code'></a>\n",
    "## 17. Code Examples\n",
    "\n",
    "### Example 1: Basic Transformer Block (Pre-Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Pre-Norm transformer block\n",
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Normalization\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # RMS normalization\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        x_normed = x / rms\n",
    "        return self.weight * x_normed\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard Multi-Head Attention\"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project and reshape\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q = q.transpose(1, 2)  # (batch, n_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Standard FFN with SwiGLU activation\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w3 = nn.Linear(d_ff, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Standard Pre-Norm Transformer Block (Llama-style)\"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-Norm: Normalize before attention\n",
    "        x = x + self.attention(self.norm1(x), mask)\n",
    "        # Pre-Norm: Normalize before FFN\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "d_model, n_heads, d_ff = 512, 8, 2048\n",
    "block = TransformerBlock(d_model, n_heads, d_ff)\n",
    "x = torch.randn(2, 10, d_model)  # (batch, seq_len, d_model)\n",
    "output = block(x)\n",
    "print(\"Standard Pre-Norm transformer block\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Grouped-Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GQA: 8 query heads share 2 KV pairs\n",
      "Group size: 4\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"GQA: Multiple query heads share key-value pairs\"\"\"\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads):\n",
    "        super().__init__()\n",
    "        assert n_heads % n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.group_size = n_heads // n_kv_heads\n",
    "        \n",
    "        # Query projection for all heads\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        # Key and Value projections only for KV heads\n",
    "        self.k_proj = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project and reshape\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose for attention\n",
    "        q = q.transpose(1, 2)  # (batch, n_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2)  # (batch, n_kv_heads, seq_len, head_dim)\n",
    "        v = v.transpose(1, 2)  # (batch, n_kv_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Expand k, v to match number of query heads\n",
    "        k = k.repeat_interleave(self.group_size, dim=1)\n",
    "        v = v.repeat_interleave(self.group_size, dim=1)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.o_proj(out)\n",
    "\n",
    "# Example usage\n",
    "d_model, n_heads, n_kv_heads = 512, 8, 2\n",
    "gqa = GroupedQueryAttention(d_model, n_heads, n_kv_heads)\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = gqa(x)\n",
    "print(f\"GQA: {n_heads} query heads share {n_kv_heads} KV pairs\")\n",
    "print(f\"Group size: {n_heads // n_kv_heads}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Sliding Window Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention Mask:\n",
      "1 = can attend, 0 = cannot attend\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "\n",
      "Window size: 3\n",
      "Each token sees at most 4 tokens (including itself)\n"
     ]
    }
   ],
   "source": [
    "def create_sliding_window_mask(seq_len, window_size):\n",
    "    \"\"\"\n",
    "    Create causal sliding window attention mask\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        window_size: Size of sliding window\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) boolean mask\n",
    "    \"\"\"\n",
    "    # Start with causal mask\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    # Apply sliding window\n",
    "    for i in range(seq_len):\n",
    "        # Token at position i can only see [i-window_size, i]\n",
    "        if i > window_size:\n",
    "            mask[i, :i-window_size] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Visualize\n",
    "seq_len, window_size = 10, 3\n",
    "mask = create_sliding_window_mask(seq_len, window_size)\n",
    "\n",
    "print(\"Sliding Window Attention Mask:\")\n",
    "print(\"1 = can attend, 0 = cannot attend\")\n",
    "print(mask.int())\n",
    "print(f\"\\nWindow size: {window_size}\")\n",
    "print(f\"Each token sees at most {window_size + 1} tokens (including itself)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: SwiGLU Activation (Standard in 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "SwiGLU replaces traditional FFN with gated activation\n"
     ]
    }
   ],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU: Swish-Gated Linear Unit\n",
    "    Used in modern LLMs (DeepSeek, Llama, Qwen, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # Two parallel projections\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)  # Gate\n",
    "        self.w2 = nn.Linear(d_model, d_ff, bias=False)  # Up projection\n",
    "        self.w3 = nn.Linear(d_ff, d_model, bias=False)  # Down projection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # SwiGLU(x) = (Swish(W1·x) ⊗ W2·x) · W3\n",
    "        # where Swish(x) = x * sigmoid(x)\n",
    "        gate = torch.nn.functional.silu(self.w1(x))  # Swish/SiLU\n",
    "        x = gate * self.w2(x)  # Element-wise multiplication (gating)\n",
    "        return self.w3(x)\n",
    "\n",
    "# Example\n",
    "d_model, d_ff = 512, 2048\n",
    "swiglu = SwiGLU(d_model, d_ff)\n",
    "x = torch.randn(2, 10, d_model)  # (batch, seq_len, d_model)\n",
    "output = swiglu(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"SwiGLU replaces traditional FFN with gated activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Simple MoE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE with 8 experts, 2 active per token\n",
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Efficiency: Only 2/8 experts active!\n"
     ]
    }
   ],
   "source": [
    "class SimpleMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Mixture-of-Experts implementation\n",
    "    Used in DeepSeek, Llama 4, Qwen3, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, num_experts, num_experts_per_token):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_experts_per_token = num_experts_per_token\n",
    "        \n",
    "        # Router: decides which experts to use\n",
    "        self.router = nn.Linear(d_model, num_experts, bias=False)\n",
    "        \n",
    "        # Expert networks (simplified as single linear layers)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff, bias=False),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(d_ff, d_model, bias=False)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Flatten for routing\n",
    "        x_flat = x.view(-1, d_model)  # (batch * seq_len, d_model)\n",
    "        \n",
    "        # Router scores\n",
    "        router_logits = self.router(x_flat)  # (batch * seq_len, num_experts)\n",
    "        router_probs = torch.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_probs, top_k_indices = torch.topk(\n",
    "            router_probs, self.num_experts_per_token, dim=-1\n",
    "        )\n",
    "        # Normalize selected expert weights\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Apply experts\n",
    "        output = torch.zeros_like(x_flat)\n",
    "        for i in range(self.num_experts_per_token):\n",
    "            expert_idx = top_k_indices[:, i]\n",
    "            expert_weight = top_k_probs[:, i:i+1]\n",
    "            \n",
    "            # Apply each selected expert\n",
    "            for expert_id in range(self.num_experts):\n",
    "                mask = (expert_idx == expert_id)\n",
    "                if mask.any():\n",
    "                    expert_input = x_flat[mask]\n",
    "                    expert_output = self.experts[expert_id](expert_input)\n",
    "                    output[mask] += expert_weight[mask] * expert_output\n",
    "        \n",
    "        # Reshape back\n",
    "        return output.view(batch_size, seq_len, d_model)\n",
    "\n",
    "# Example: DeepSeek-style MoE\n",
    "d_model, d_ff = 512, 2048\n",
    "num_experts, active_experts = 8, 2\n",
    "moe = SimpleMoE(d_model, d_ff, num_experts, active_experts)\n",
    "\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = moe(x)\n",
    "print(f\"MoE with {num_experts} experts, {active_experts} active per token\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nEfficiency: Only {active_experts}/{num_experts} experts active!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: RoPE (Rotary Position Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoPE applied to queries\n",
      "Original shape: torch.Size([2, 8, 128, 64])\n",
      "Rotated shape: torch.Size([2, 8, 128, 64])\n",
      "\n",
      "RoPE encodes position through rotation, no learned parameters!\n"
     ]
    }
   ],
   "source": [
    "def precompute_rope_params(head_dim, seq_len, theta=10000.0):\n",
    "    \"\"\"\n",
    "    Precompute RoPE rotation parameters\n",
    "    Used by: Llama, Qwen, Mistral, and most modern LLMs\n",
    "    \"\"\"\n",
    "    # Compute frequencies\n",
    "    freq = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "    \n",
    "    # Create position indices\n",
    "    positions = torch.arange(seq_len)\n",
    "    \n",
    "    # Compute angles\n",
    "    angles = torch.outer(positions, freq)  # (seq_len, head_dim // 2)\n",
    "    \n",
    "    # Compute cos and sin\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    \n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    \"\"\"\n",
    "    Apply rotary position embedding to input tensor\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (..., seq_len, head_dim)\n",
    "        cos, sin: Precomputed rotation parameters\n",
    "    \"\"\"\n",
    "    # Split into even and odd dimensions\n",
    "    x1 = x[..., ::2]  # Even indices\n",
    "    x2 = x[..., 1::2]  # Odd indices\n",
    "    \n",
    "    # Apply rotation\n",
    "    # Reshape cos/sin to match x dimensions\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, head_dim//2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    rotated_x1 = x1 * cos - x2 * sin\n",
    "    rotated_x2 = x1 * sin + x2 * cos\n",
    "    \n",
    "    # Interleave back\n",
    "    rotated_x = torch.stack([rotated_x1, rotated_x2], dim=-1)\n",
    "    rotated_x = rotated_x.flatten(-2)\n",
    "    \n",
    "    return rotated_x\n",
    "\n",
    "# Example usage\n",
    "head_dim, seq_len = 64, 128\n",
    "cos, sin = precompute_rope_params(head_dim, seq_len)\n",
    "\n",
    "# Simulate query/key tensor\n",
    "q = torch.randn(2, 8, seq_len, head_dim)  # (batch, heads, seq_len, head_dim)\n",
    "q_rotated = apply_rope(q, cos, sin)\n",
    "\n",
    "print(f\"RoPE applied to queries\")\n",
    "print(f\"Original shape: {q.shape}\")\n",
    "print(f\"Rotated shape: {q_rotated.shape}\")\n",
    "print(f\"\\nRoPE encodes position through rotation, no learned parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Memory Efficiency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache Memory Comparison (4k context, fp16):\n",
      "\n",
      "Model Type                     Heads      Memory (GB)    \n",
      "-------------------------------------------------------\n",
      "MHA (all heads)                32         2.00           \n",
      "GQA (8 KV heads)               8          0.50           \n",
      "MLA (compressed)               N/A        1.00           \n",
      "\n",
      "=======================================================\n",
      "GQA saves: 75.0% vs MHA\n",
      "MLA saves: 50.0% vs MHA\n",
      "\n",
      "=======================================================\n",
      "Impact of Long Context (32k tokens):\n",
      "\n",
      "GQA at 32k context: 4.00 GB\n",
      "That's 8.0x more memory than 4k context!\n"
     ]
    }
   ],
   "source": [
    "def calculate_kv_cache_size(seq_len, n_layers, n_heads, head_dim, \n",
    "                           batch_size=1, dtype_bytes=2):\n",
    "    \"\"\"\n",
    "    Calculate KV cache memory for different attention mechanisms\n",
    "    dtype_bytes: 2 for fp16/bf16, 4 for fp32\n",
    "    \"\"\"\n",
    "    # Each position stores K and V for each layer and head\n",
    "    kv_cache_bytes = (\n",
    "        2 *  # K and V\n",
    "        batch_size * \n",
    "        seq_len * \n",
    "        n_layers * \n",
    "        n_heads * \n",
    "        head_dim * \n",
    "        dtype_bytes\n",
    "    )\n",
    "    return kv_cache_bytes / (1024**3)  # Convert to GB\n",
    "\n",
    "# Example: Compare different models\n",
    "seq_len = 4096  # 4k context\n",
    "n_layers = 32\n",
    "head_dim = 128\n",
    "\n",
    "print(\"KV Cache Memory Comparison (4k context, fp16):\\n\")\n",
    "print(f\"{'Model Type':<30} {'Heads':<10} {'Memory (GB)':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Multi-Head Attention (traditional)\n",
    "mha_heads = 32\n",
    "mha_mem = calculate_kv_cache_size(seq_len, n_layers, mha_heads, head_dim)\n",
    "print(f\"{'MHA (all heads)':<30} {mha_heads:<10} {mha_mem:<15.2f}\")\n",
    "\n",
    "# Grouped-Query Attention\n",
    "gqa_kv_heads = 8  # 4 groups\n",
    "gqa_mem = calculate_kv_cache_size(seq_len, n_layers, gqa_kv_heads, head_dim)\n",
    "print(f\"{'GQA (8 KV heads)':<30} {gqa_kv_heads:<10} {gqa_mem:<15.2f}\")\n",
    "\n",
    "# Multi-Head Latent Attention (compressed)\n",
    "mla_compression = 0.5  # Roughly 50% compression\n",
    "mla_mem = mha_mem * mla_compression\n",
    "print(f\"{'MLA (compressed)':<30} {'N/A':<10} {mla_mem:<15.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(f\"GQA saves: {(1 - gqa_mem/mha_mem)*100:.1f}% vs MHA\")\n",
    "print(f\"MLA saves: {(1 - mla_mem/mha_mem)*100:.1f}% vs MHA\")\n",
    "\n",
    "# Longer context comparison\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"Impact of Long Context (32k tokens):\\n\")\n",
    "long_seq = 32768\n",
    "gqa_long = calculate_kv_cache_size(long_seq, n_layers, gqa_kv_heads, head_dim)\n",
    "print(f\"GQA at 32k context: {gqa_long:.2f} GB\")\n",
    "print(f\"That's {gqa_long/gqa_mem:.1f}x more memory than 4k context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: MoE Parameter Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE Efficiency Comparison:\n",
      "\n",
      "Model                Total (B)    Active (B)   Efficiency     \n",
      "-----------------------------------------------------------------\n",
      "DeepSeek V3          460.3        16.1         3.5%\n",
      "Llama 4 Maverick     171.8        21.5         12.5%\n",
      "Qwen3-Next           412.7        3.6          0.9%\n",
      "\n",
      "=================================================================\n",
      "Key Insight: MoE models use only 5-20% of parameters during inference!\n",
      "This allows massive models with reasonable inference costs.\n"
     ]
    }
   ],
   "source": [
    "def calculate_moe_params(d_model, d_ff, n_layers, num_experts, \n",
    "                        experts_per_token, has_shared_expert=False):\n",
    "    \"\"\"\n",
    "    Calculate total vs active parameters for MoE models\n",
    "    \"\"\"\n",
    "    # Parameters per expert (simplified: up + down projection)\n",
    "    params_per_expert = d_model * d_ff * 2\n",
    "    \n",
    "    # Shared expert (if present)\n",
    "    shared_params = params_per_expert if has_shared_expert else 0\n",
    "    \n",
    "    # Total MoE parameters per layer\n",
    "    total_per_layer = (params_per_expert * num_experts) + shared_params\n",
    "    \n",
    "    # Active parameters per token\n",
    "    active_per_layer = (params_per_expert * experts_per_token) + shared_params\n",
    "    \n",
    "    # Total across all layers\n",
    "    total_params = total_per_layer * n_layers\n",
    "    active_params = active_per_layer * n_layers\n",
    "    \n",
    "    return total_params / 1e9, active_params / 1e9  # Return in billions\n",
    "\n",
    "print(\"MoE Efficiency Comparison:\\n\")\n",
    "print(f\"{'Model':<20} {'Total (B)':<12} {'Active (B)':<12} {'Efficiency':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# DeepSeek V3 configuration\n",
    "total, active = calculate_moe_params(\n",
    "    d_model=7168, d_ff=2048, n_layers=61,\n",
    "    num_experts=256, experts_per_token=8,\n",
    "    has_shared_expert=True\n",
    ")\n",
    "print(f\"{'DeepSeek V3':<20} {total:<12.1f} {active:<12.1f} {(active/total)*100:<.1f}%\")\n",
    "\n",
    "# Llama 4 Maverick configuration (approximate)\n",
    "total, active = calculate_moe_params(\n",
    "    d_model=8192, d_ff=8192, n_layers=80,\n",
    "    num_experts=16, experts_per_token=2,\n",
    "    has_shared_expert=False\n",
    ")\n",
    "print(f\"{'Llama 4 Maverick':<20} {total:<12.1f} {active:<12.1f} {(active/total)*100:<.1f}%\")\n",
    "\n",
    "# Qwen3-Next (1024 experts!)\n",
    "total, active = calculate_moe_params(\n",
    "    d_model=4096, d_ff=1024, n_layers=48,\n",
    "    num_experts=1024, experts_per_token=8,\n",
    "    has_shared_expert=True\n",
    ")\n",
    "print(f\"{'Qwen3-Next':<20} {total:<12.1f} {active:<12.1f} {(active/total)*100:<.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"Key Insight: MoE models use only 5-20% of parameters during inference!\")\n",
    "print(\"This allows massive models with reasonable inference costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9: Comparing Width vs Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width vs Depth Trade-off Analysis:\n",
      "\n",
      "Target: ~3B parameters\n",
      "\n",
      "Strategy        Layers     d_model    d_ff       Total (B)   \n",
      "-----------------------------------------------------------------\n",
      "Deeper          48         2048       5632       2.02        \n",
      "Wider           24         2880       7680       2.00        \n",
      "\n",
      "=================================================================\n",
      "Trade-offs:\n",
      "\n",
      "Deeper (More Layers):\n",
      "  ✓ More flexible representations\n",
      "  ✓ Lower memory per layer\n",
      "  ✗ Slower inference (sequential)\n",
      "  ✗ Harder to train (gradients)\n",
      "\n",
      "Wider (Larger d_model):\n",
      "  ✓ Faster inference (parallel)\n",
      "  ✓ Better tokens/second\n",
      "  ✗ Higher memory usage\n",
      "  ✗ Less layer-wise specialization\n"
     ]
    }
   ],
   "source": [
    "def calculate_model_params(n_layers, d_model, n_heads, d_ff, vocab_size=50000):\n",
    "    \"\"\"\n",
    "    Rough parameter count calculation for transformer model\n",
    "    \"\"\"\n",
    "    head_dim = d_model // n_heads\n",
    "    \n",
    "    # Embedding\n",
    "    embed_params = vocab_size * d_model\n",
    "    \n",
    "    # Per layer\n",
    "    # Attention: Q, K, V, O projections\n",
    "    attn_params = 4 * d_model * d_model\n",
    "    # FFN: up + down\n",
    "    ffn_params = 2 * d_model * d_ff\n",
    "    # Layer params\n",
    "    layer_params = attn_params + ffn_params\n",
    "    \n",
    "    total = embed_params + (n_layers * layer_params)\n",
    "    return total / 1e9  # Billions\n",
    "\n",
    "print(\"Width vs Depth Trade-off Analysis:\\n\")\n",
    "print(\"Target: ~3B parameters\\n\")\n",
    "print(f\"{'Strategy':<15} {'Layers':<10} {'d_model':<10} {'d_ff':<10} {'Total (B)':<12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Deeper model (Qwen3-style)\n",
    "deeper = calculate_model_params(\n",
    "    n_layers=48, d_model=2048, n_heads=16, d_ff=5632\n",
    ")\n",
    "print(f\"{'Deeper':<15} {48:<10} {2048:<10} {5632:<10} {deeper:<12.2f}\")\n",
    "\n",
    "# Wider model (gpt-oss-style)\n",
    "wider = calculate_model_params(\n",
    "    n_layers=24, d_model=2880, n_heads=24, d_ff=7680\n",
    ")\n",
    "print(f\"{'Wider':<15} {24:<10} {2880:<10} {7680:<10} {wider:<12.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"Trade-offs:\")\n",
    "print(\"\\nDeeper (More Layers):\")\n",
    "print(\"  ✓ More flexible representations\")\n",
    "print(\"  ✓ Lower memory per layer\")\n",
    "print(\"  ✗ Slower inference (sequential)\")\n",
    "print(\"  ✗ Harder to train (gradients)\")\n",
    "\n",
    "print(\"\\nWider (Larger d_model):\")\n",
    "print(\"  ✓ Faster inference (parallel)\")\n",
    "print(\"  ✓ Better tokens/second\")\n",
    "print(\"  ✗ Higher memory usage\")\n",
    "print(\"  ✗ Less layer-wise specialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10: Visualization - Attention Pattern Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention patterns visualized!\n",
      "Saved as 'attention_patterns.png'\n",
      "\n",
      "Memory comparison (relative):\n",
      "Full attention: 210 elements (100%)\n",
      "Sliding window: 105 elements (50.0%)\n",
      "Sparse pattern: 90 elements (42.9%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention_patterns():\n",
    "    \"\"\"\n",
    "    Visualize different attention patterns:\n",
    "    - Full (causal) attention\n",
    "    - Sliding window attention\n",
    "    - Sparse attention\n",
    "    \"\"\"\n",
    "    seq_len = 20\n",
    "    \n",
    "    # Create attention masks\n",
    "    # Full causal attention\n",
    "    full_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    \n",
    "    # Sliding window (window=5)\n",
    "    sliding_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    for i in range(seq_len):\n",
    "        if i > 5:\n",
    "            sliding_mask[i, :i-5] = 0\n",
    "    \n",
    "    # Sparse (example pattern)\n",
    "    sparse_mask = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        # Can attend to itself\n",
    "        sparse_mask[i, i] = 1\n",
    "        # Can attend to previous positions (stride=3)\n",
    "        for j in range(0, i, 3):\n",
    "            sparse_mask[i, j] = 1\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    patterns = [\n",
    "        (full_mask, \"Full Causal Attention\\n(Standard)\"),\n",
    "        (sliding_mask, \"Sliding Window Attention\\n(Gemma 3, window=5)\"),\n",
    "        (sparse_mask, \"Sparse Attention\\n(Example Pattern)\")\n",
    "    ]\n",
    "    \n",
    "    for ax, (mask, title) in zip(axes, patterns):\n",
    "        im = ax.imshow(mask, cmap='Blues', aspect='auto')\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        ax.set_xticks(range(0, seq_len, 5))\n",
    "        ax.set_yticks(range(0, seq_len, 5))\n",
    "        \n",
    "        # Add grid\n",
    "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attention_patterns.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Attention patterns visualized!\")\n",
    "    print(\"Saved as 'attention_patterns.png'\")\n",
    "    \n",
    "    # Calculate memory savings\n",
    "    full_size = np.sum(full_mask)\n",
    "    sliding_size = np.sum(sliding_mask)\n",
    "    sparse_size = np.sum(sparse_mask)\n",
    "    \n",
    "    print(f\"\\nMemory comparison (relative):\")\n",
    "    print(f\"Full attention: {full_size:.0f} elements (100%)\")\n",
    "    print(f\"Sliding window: {sliding_size:.0f} elements ({sliding_size/full_size*100:.1f}%)\")\n",
    "    print(f\"Sparse pattern: {sparse_size:.0f} elements ({sparse_size/full_size*100:.1f}%)\")\n",
    "\n",
    "# Run visualization\n",
    "try:\n",
    "    visualize_attention_patterns()\n",
    "except ImportError:\n",
    "    print(\"Install matplotlib to see visualizations: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 11: Model Architecture Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:48: SyntaxWarning: invalid escape sequence '\\+'\n",
      "/tmp/ipykernel_17607/1891909911.py:48: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  print(f\"   Hybrid approaches: {(df['Attention'].str.contains('\\+')).sum()} models\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "            Model  Total_Params_B  Active_Params_B   Attention MoE Shared_Expert          Use_Case  Efficiency_%\n",
      "      DeepSeek V3             671             37.0         MLA Yes           Yes General/Reasoning           5.5\n",
      " Llama 4 Maverick             400             17.0         GQA Yes            No           General           4.2\n",
      "  Qwen3 Dense 32B              32             32.0         GQA  No           N/A   Local/Fine-tune         100.0\n",
      "   Qwen3 MoE 235B             235             22.0         GQA Yes            No        Production           9.4\n",
      "   Qwen3-Next 80B              80              3.0 DeltaNet+GA Yes           Yes      Long Context           3.8\n",
      "      Gemma 3 27B              27             27.0 GQA+Sliding  No           N/A          Balanced         100.0\n",
      "Mistral Small 24B              24             24.0         GQA  No           N/A             Speed         100.0\n",
      "       SmolLM3 3B               3              3.0         GQA  No           N/A        Small/Edge         100.0\n",
      "          Kimi K2            1000             45.0         MLA Yes           Yes          Flagship           4.5\n",
      "      gpt-oss 20B              20              3.6 GQA+Sliding Yes            No          Research          18.0\n",
      "\n",
      "================================================================================\n",
      "KEY OBSERVATIONS:\n",
      "================================================================================\n",
      "\n",
      "1. MoE Efficiency:\n",
      "   Average efficiency for MoE models: 7.6%\n",
      "   This means only ~8% of parameters used during inference!\n",
      "\n",
      "2. Attention Mechanisms:\n",
      "   GQA variants: 7 models\n",
      "   MLA: 2 models\n",
      "   Hybrid approaches: 3 models\n",
      "\n",
      "3. Size Distribution:\n",
      "   Small (<10B): 1 models\n",
      "   Medium (10-100B): 5 models\n",
      "   Large (100-500B): 2 models\n",
      "   Massive (>500B): 2 models\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comprehensive comparison\n",
    "models_data = {\n",
    "    'Model': [\n",
    "        'DeepSeek V3', 'Llama 4 Maverick', 'Qwen3 Dense 32B',\n",
    "        'Qwen3 MoE 235B', 'Qwen3-Next 80B', 'Gemma 3 27B',\n",
    "        'Mistral Small 24B', 'SmolLM3 3B', 'Kimi K2', 'gpt-oss 20B'\n",
    "    ],\n",
    "    'Total_Params_B': [671, 400, 32, 235, 80, 27, 24, 3, 1000, 20],\n",
    "    'Active_Params_B': [37, 17, 32, 22, 3, 27, 24, 3, 45, 3.6],\n",
    "    'Attention': [\n",
    "        'MLA', 'GQA', 'GQA', 'GQA', 'DeltaNet+GA', 'GQA+Sliding',\n",
    "        'GQA', 'GQA', 'MLA', 'GQA+Sliding'\n",
    "    ],\n",
    "    'MoE': ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes'],\n",
    "    'Shared_Expert': ['Yes', 'No', 'N/A', 'No', 'Yes', 'N/A', 'N/A', 'N/A', 'Yes', 'No'],\n",
    "    'Use_Case': [\n",
    "        'General/Reasoning', 'General', 'Local/Fine-tune', \n",
    "        'Production', 'Long Context', 'Balanced',\n",
    "        'Speed', 'Small/Edge', 'Flagship', 'Research'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(models_data)\n",
    "\n",
    "# Calculate efficiency ratio\n",
    "df['Efficiency_%'] = (df['Active_Params_B'] / df['Total_Params_B'] * 100).round(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY OBSERVATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. MoE Efficiency:\")\n",
    "moe_models = df[df['MoE'] == 'Yes']\n",
    "print(f\"   Average efficiency for MoE models: {moe_models['Efficiency_%'].mean():.1f}%\")\n",
    "print(f\"   This means only ~{moe_models['Efficiency_%'].mean():.0f}% of parameters used during inference!\")\n",
    "\n",
    "print(\"\\n2. Attention Mechanisms:\")\n",
    "print(f\"   GQA variants: {(df['Attention'].str.contains('GQA').sum())} models\")\n",
    "print(f\"   MLA: {(df['Attention'] == 'MLA').sum()} models\")\n",
    "print(f\"   Hybrid approaches: {(df['Attention'].str.contains('\\+')).sum()} models\")\n",
    "\n",
    "print(\"\\n3. Size Distribution:\")\n",
    "print(f\"   Small (<10B): {(df['Total_Params_B'] < 10).sum()} models\")\n",
    "print(f\"   Medium (10-100B): {((df['Total_Params_B'] >= 10) & (df['Total_Params_B'] < 100)).sum()} models\")\n",
    "print(f\"   Large (100-500B): {((df['Total_Params_B'] >= 100) & (df['Total_Params_B'] < 500)).sum()} models\")\n",
    "print(f\"   Massive (>500B): {(df['Total_Params_B'] >= 500).sum()} models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "## 18. Further Reading & Resources\n",
    "\n",
    "### Original Article\n",
    "- **The Big LLM Architecture Comparison** by Sebastian Raschka\n",
    "- https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\n",
    "\n",
    "### Key Papers Referenced\n",
    "\n",
    "**Architecture Papers:**\n",
    "1. **DeepSeek V3**: https://arxiv.org/abs/2405.04434\n",
    "2. **OLMo 2**: https://arxiv.org/abs/2501.00656\n",
    "3. **Gemma 3**: https://arxiv.org/abs/2503.19786\n",
    "4. **Grouped-Query Attention**: Search for GQA papers\n",
    "5. **NoPE (No Position Embedding)**: https://arxiv.org/abs/2305.19466\n",
    "6. **DeepSeekMoE**: https://arxiv.org/abs/2401.06066\n",
    "\n",
    "**Foundational:**\n",
    "- Attention Is All You Need (Original Transformer)\n",
    "- GPT-2 Paper (Pre-LN normalization)\n",
    "- Llama Papers (RoPE, RMSNorm, GQA)\n",
    "\n",
    "### Community Resources\n",
    "\n",
    "**Model Implementations:**\n",
    "- Hugging Face Transformers library\n",
    "- Individual model repositories on GitHub\n",
    "\n",
    "**Sebastian Raschka's Resources:**\n",
    "- Book: *Build A Large Language Model (From Scratch)*\n",
    "- Magazine: https://magazine.sebastianraschka.com/\n",
    "- From-scratch implementations and guides\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tips'></a>\n",
    "## 19. Study Tips\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "**For Quick Review:**\n",
    "1. Read Key Takeaways (Section 14)\n",
    "2. Check Comparison Table (Section 15)\n",
    "3. Review diagrams and summaries\n",
    "\n",
    "**For Deep Understanding:**\n",
    "1. Start with Introduction (Section 1)\n",
    "2. Work through each model sequentially\n",
    "3. Run code examples\n",
    "4. Compare similar architectures\n",
    "\n",
    "**For Practical Application:**\n",
    "1. Read Practical Implications (Section 16)\n",
    "2. Study code examples (Section 17)\n",
    "3. Implement components yourself\n",
    "\n",
    "### Focus Areas by Goal\n",
    "\n",
    "**If you're interested in:**\n",
    "\n",
    "- **Efficiency**: Focus on MoE, GQA/MLA, Sliding Window\n",
    "- **Performance**: Study DeepSeek V3, Kimi 2, MLA\n",
    "- **Small models**: Deep dive into Qwen3 0.6B, SmolLM3\n",
    "- **Long context**: Qwen3-Next DeltaNet, Sliding Window\n",
    "- **Training stability**: OLMo 2, Gemma 3 normalization\n",
    "- **Implementation**: Code examples, architectural patterns\n",
    "\n",
    "### Key Concepts to Master\n",
    "\n",
    "1. ✅ **Attention mechanisms**: MHA → GQA → MLA\n",
    "2. ✅ **MoE fundamentals**: Sparse vs dense, routing, shared experts\n",
    "3. ✅ **Normalization**: RMSNorm, Pre/Post-Norm, QK-Norm\n",
    "4. ✅ **Position encoding**: RoPE vs NoPE\n",
    "5. ✅ **Efficiency trade-offs**: Memory vs compute vs performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 20. Practice Exercises\n",
    "\n",
    "Test your understanding with these exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate MoE Savings\n",
    "\n",
    "**Problem**: A model has:\n",
    "- 128 experts\n",
    "- 4 active experts per token\n",
    "- Each expert has 1B parameters\n",
    "- 1 shared expert (always active)\n",
    "\n",
    "Calculate:\n",
    "1. Total parameters\n",
    "2. Active parameters per token\n",
    "3. Efficiency percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 129B\n",
      "Active parameters: 5B\n",
      "Efficiency: 3.9%\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "num_experts = 128\n",
    "active_experts = 4\n",
    "params_per_expert = 1  # billion\n",
    "has_shared_expert = True\n",
    "\n",
    "# Calculate\n",
    "total_params = num_experts * params_per_expert\n",
    "active_params = active_experts * params_per_expert\n",
    "\n",
    "if has_shared_expert:\n",
    "    total_params += params_per_expert  # Add shared expert\n",
    "    active_params += params_per_expert  # Shared expert always active\n",
    "\n",
    "efficiency = (active_params / total_params) * 100\n",
    "\n",
    "print(f\"Total parameters: {total_params}B\")\n",
    "print(f\"Active parameters: {active_params}B\")\n",
    "print(f\"Efficiency: {efficiency:.1f}%\")\n",
    "\n",
    "# Expected: 129B total, 5B active, ~3.9% efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Attention Mask Creation\n",
    "\n",
    "**Problem**: Create a causal attention mask with a sliding window of size 4 for sequence length 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding Window Causal Mask (window=4):\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "def create_sliding_causal_mask(seq_len, window_size):\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        if i > window_size:\n",
    "            mask[i, :i-window_size] = 0\n",
    "    return mask\n",
    "\n",
    "mask = create_sliding_causal_mask(8, 4)\n",
    "print(\"Sliding Window Causal Mask (window=4):\")\n",
    "print(mask.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: GQA Parameter Count\n",
    "\n",
    "**Problem**: Compare parameter counts:\n",
    "- Model with MHA: 32 heads\n",
    "- Model with GQA: 32 query heads, 8 KV heads\n",
    "- d_model = 4096\n",
    "\n",
    "How many parameters are saved in the attention layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA parameters: 67.1M\n",
      "GQA parameters: 41.9M\n",
      "Parameters saved: 25.2M (37.5%)\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "d_model = 4096\n",
    "n_heads = 32\n",
    "n_kv_heads = 8\n",
    "\n",
    "# MHA: Q, K, V, O projections (all use d_model x d_model)\n",
    "mha_params = 4 * (d_model * d_model)\n",
    "\n",
    "# GQA: Q and O are same, but K and V are smaller\n",
    "head_dim = d_model // n_heads\n",
    "gqa_params = (\n",
    "    d_model * d_model +  # Q projection\n",
    "    d_model * (n_kv_heads * head_dim) +  # K projection (smaller)\n",
    "    d_model * (n_kv_heads * head_dim) +  # V projection (smaller)\n",
    "    d_model * d_model  # O projection\n",
    ")\n",
    "\n",
    "saved = mha_params - gqa_params\n",
    "percentage = (saved / mha_params) * 100\n",
    "\n",
    "print(f\"MHA parameters: {mha_params/1e6:.1f}M\")\n",
    "print(f\"GQA parameters: {gqa_params/1e6:.1f}M\")\n",
    "print(f\"Parameters saved: {saved/1e6:.1f}M ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quiz'></a>\n",
    "## 21. Quick Quiz\n",
    "\n",
    "### Test Your Knowledge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: Which attention mechanism compresses K and V tensors into lower-dimensional space?\n",
    "\n",
    "A) GQA  \n",
    "B) MLA  \n",
    "C) Sliding Window  \n",
    "D) DeltaNet  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: B) MLA (Multi-Head Latent Attention)</b><br>\n",
    "Used by DeepSeek V3 and Kimi K2. It compresses K,V before storing in KV cache, then projects back during inference.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2**: What is the main benefit of having a shared expert in MoE?\n",
    "\n",
    "A) Reduces total parameters  \n",
    "B) Speeds up training  \n",
    "C) Learns common patterns once  \n",
    "D) Improves routing decisions  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: C) Learns common patterns once</b><br>\n",
    "Common/repeated patterns don't need to be learned by multiple individual experts, leaving them room for specialization.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 3**: Which model uses NO positional embeddings (NoPE) in some layers?\n",
    "\n",
    "A) Llama 4  \n",
    "B) SmolLM3  \n",
    "C) Gemma 3  \n",
    "D) DeepSeek V3  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: B) SmolLM3</b><br>\n",
    "Uses NoPE in every 4th layer. Relies on causal attention mask for implicit positional information.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 4**: What is the ratio of sliding window to full attention layers in Gemma 3?\n",
    "\n",
    "A) 1:1  \n",
    "B) 2:1  \n",
    "C) 5:1  \n",
    "D) 10:1  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: C) 5:1</b><br>\n",
    "Gemma 3 uses 5 sliding window layers for every 1 full attention layer. This is more aggressive than Gemma 2's 1:1 ratio.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 5**: Which normalization placement does OLMo 2 use?\n",
    "\n",
    "A) Pre-Norm only  \n",
    "B) Post-Norm only  \n",
    "C) Modified Post-Norm (inside residual)  \n",
    "D) Pre-Norm and Post-Norm both  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: C) Modified Post-Norm (inside residual)</b><br>\n",
    "Normalization is placed AFTER attention/FFN but still INSIDE the residual connections, unlike traditional Post-Norm.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 6**: Qwen3-Next uses a hybrid of which two mechanisms?\n",
    "\n",
    "A) MHA and GQA  \n",
    "B) GQA and MLA  \n",
    "C) DeltaNet and Gated Attention  \n",
    "D) Sliding Window and Full Attention  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: C) DeltaNet and Gated Attention</b><br>\n",
    "Uses 75% DeltaNet layers (linear-time) and 25% Gated Attention layers (for precision) in a 3:1 ratio.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 7**: How many experts does Qwen3-Next have?\n",
    "\n",
    "A) 64  \n",
    "B) 256  \n",
    "C) 512  \n",
    "D) 1024  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: D) 1024</b><br>\n",
    "Qwen3-Next has an impressive 1024 experts! This is 4x more than the original Qwen3 235B model.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 8**: Which activation function is standard across all 2025 models?\n",
    "\n",
    "A) ReLU  \n",
    "B) GELU  \n",
    "C) SwiGLU  \n",
    "D) Tanh  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: C) SwiGLU</b><br>\n",
    "SwiGLU (Swish-Gated Linear Unit) has replaced GELU and ReLU in modern LLMs. Used by all models in this comparison.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 9**: Why does GLM-4.5 use dense layers before MoE layers?\n",
    "\n",
    "A) To reduce total parameters  \n",
    "B) For better training stability  \n",
    "C) To speed up inference  \n",
    "D) To save memory  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: B) For better training stability</b><br>\n",
    "First 3 layers are dense to establish stable low-level features before MoE routing begins. Improves convergence.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 10**: DeepSeek V3 has 671B total parameters. How many are active during inference?\n",
    "\n",
    "A) 17B  \n",
    "B) 37B  \n",
    "C) 67B  \n",
    "D) 134B  \n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<b>Answer: B) 37B</b><br>\n",
    "Only 37B parameters active (5.5% of total). Uses 8 routed experts + 1 shared expert out of 256 total experts.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='checklist'></a>\n",
    "## 22. Revision Checklist\n",
    "\n",
    "### Core Concepts to Master\n",
    "\n",
    "Use this checklist to track your understanding:\n",
    "\n",
    "**Attention Mechanisms:**\n",
    "- [ ] Understand Multi-Head Attention (MHA) basics\n",
    "- [ ] Explain Grouped-Query Attention (GQA) and how it differs from MHA\n",
    "- [ ] Describe Multi-Head Latent Attention (MLA) compression strategy\n",
    "- [ ] Understand sliding window attention and its memory benefits\n",
    "- [ ] Know when to use each attention type\n",
    "\n",
    "**Mixture-of-Experts (MoE):**\n",
    "- [ ] Understand sparse vs dense models\n",
    "- [ ] Explain how routing works in MoE\n",
    "- [ ] Describe the role of shared experts\n",
    "- [ ] Calculate total vs active parameters\n",
    "- [ ] Understand the trend toward many small experts\n",
    "\n",
    "**Normalization:**\n",
    "- [ ] Explain RMSNorm vs LayerNorm\n",
    "- [ ] Understand Pre-Norm vs Post-Norm placement\n",
    "- [ ] Describe QK-Norm and its benefits\n",
    "- [ ] Know which models use which strategies\n",
    "\n",
    "**Position Encoding:**\n",
    "- [ ] Understand RoPE (Rotary Position Embedding)\n",
    "- [ ] Explain NoPE (No Position Embedding) approach\n",
    "- [ ] Know why RoPE is the standard choice\n",
    "\n",
    "**Model Architectures:**\n",
    "- [ ] Compare DeepSeek V3 and Llama 4 architectures\n",
    "- [ ] Understand Gemma 3's sliding window strategy\n",
    "- [ ] Know Qwen3-Next's DeltaNet hybrid approach\n",
    "- [ ] Describe Kimi K2's scale and innovations\n",
    "\n",
    "**Practical Understanding:**\n",
    "- [ ] Calculate KV cache memory for different attention types\n",
    "- [ ] Estimate MoE parameter efficiency\n",
    "- [ ] Understand width vs depth trade-offs\n",
    "- [ ] Choose appropriate model for different use cases\n",
    "\n",
    "**Coding:**\n",
    "- [ ] Implement basic RMSNorm\n",
    "- [ ] Code GQA from scratch\n",
    "- [ ] Create attention masks (causal, sliding window)\n",
    "- [ ] Implement SwiGLU activation\n",
    "- [ ] Build simple MoE layer\n",
    "\n",
    "### Progress Tracking\n",
    "\n",
    "Rate your understanding (1-5) for each major section:\n",
    "\n",
    "- DeepSeek V3/R1: ___/5\n",
    "- OLMo 2: ___/5\n",
    "- Gemma 3: ___/5\n",
    "- Llama 4: ___/5\n",
    "- Qwen3 & Qwen3-Next: ___/5\n",
    "- SmolLM3: ___/5\n",
    "- Kimi K2: ___/5\n",
    "- gpt-oss: ___/5\n",
    "- Key Trends & Takeaways: ___/5\n",
    "\n",
    "**Study Plan Recommendations:**\n",
    "- Sections rated 1-2: Need deep review, work through examples\n",
    "- Sections rated 3: Review key points and practice exercises\n",
    "- Sections rated 4-5: Quick review of summaries sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nextsteps'></a>\n",
    "## 23. Additional Resources & Next Steps\n",
    "\n",
    "### Recommended Study Path\n",
    "\n",
    "**Week 1: Foundations**\n",
    "- Review transformer basics\n",
    "- Master attention mechanisms (MHA → GQA → MLA)\n",
    "- Understand normalization strategies\n",
    "- Practice: Implement basic components\n",
    "\n",
    "**Week 2: Advanced Concepts**\n",
    "- Deep dive into MoE architectures\n",
    "- Study efficiency techniques\n",
    "- Compare model architectures\n",
    "- Practice: Calculate memory and efficiency metrics\n",
    "\n",
    "**Week 3: Practical Application**\n",
    "- Model selection for different use cases\n",
    "- Implementation best practices\n",
    "- Read original papers\n",
    "- Practice: Build complete transformer block\n",
    "\n",
    "**Week 4: Deep Understanding**\n",
    "- Analyze trade-offs\n",
    "- Understand design decisions\n",
    "- Study ablation studies from papers\n",
    "- Practice: Design your own architecture\n",
    "\n",
    "### Papers to Read (Priority Order)\n",
    "\n",
    "**Must Read:**\n",
    "1. Attention Is All You Need (Original Transformer)\n",
    "2. GPT-2 Paper (Pre-LN normalization)\n",
    "3. DeepSeek-V3 Technical Report\n",
    "4. Llama 2 Paper (GQA introduction)\n",
    "\n",
    "**Highly Recommended:**\n",
    "5. OLMo 2 Technical Report (transparency)\n",
    "6. Gemma 3 Paper (sliding window attention)\n",
    "7. DeepSeekMoE Paper (expert specialization)\n",
    "8. NoPE Paper (position embedding alternatives)\n",
    "\n",
    "**For Deep Dives:**\n",
    "9. RoPE Paper (rotary embeddings)\n",
    "10. SwiGLU Paper (activation functions)\n",
    "11. Flash Attention Papers (efficient implementation)\n",
    "12. Mixture-of-Experts foundational papers\n",
    "\n",
    "### Hands-On Practice\n",
    "\n",
    "**Beginner:**\n",
    "- Implement RMSNorm and compare with LayerNorm\n",
    "- Create attention masks for different patterns\n",
    "- Build basic transformer block\n",
    "\n",
    "**Intermediate:**\n",
    "- Implement GQA from scratch\n",
    "- Build simple MoE layer with routing\n",
    "- Compare memory usage of different attention types\n",
    "\n",
    "**Advanced:**\n",
    "- Implement sliding window attention efficiently\n",
    "- Build MLA (latent attention)\n",
    "- Create hybrid architecture combining multiple techniques\n",
    "\n",
    "### Community & Discussion\n",
    "\n",
    "- **Sebastian Raschka's Newsletter**: For latest LLM developments\n",
    "- **Hugging Face Forums**: Model discussions and implementations\n",
    "- **ArXiv**: Latest research papers\n",
    "- **GitHub**: Model implementations and code examples\n",
    "\n",
    "### Keep Learning\n",
    "\n",
    "The field is rapidly evolving! Key areas to watch:\n",
    "- Long-context handling (beyond 1M tokens)\n",
    "- More efficient MoE designs\n",
    "- Alternative attention mechanisms\n",
    "- Smaller, more efficient models\n",
    "- Multimodal architecture integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The LLM architecture landscape in 2025 shows both **convergence** and **diversity**:\n",
    "\n",
    "**Convergence:**\n",
    "- RMSNorm is standard\n",
    "- RoPE dominates positional encoding\n",
    "- SwiGLU replaces older activations\n",
    "- MoE gaining widespread adoption\n",
    "\n",
    "**Diversity:**\n",
    "- Attention: GQA vs MLA vs Sliding Window vs DeltaNet\n",
    "- Expert configuration: Few large vs many small\n",
    "- Normalization placement: Various strategies\n",
    "- Architecture dimensions: Wide vs deep\n",
    "\n",
    "**Key Insight:**\n",
    "> The fact that leading labs haven't converged to identical architectures suggests we haven't hit a ceiling yet. There's still room for innovation and improvement.\n",
    "\n",
    "**Looking Forward:**\n",
    "- MoE will likely become standard for large models\n",
    "- Attention efficiency remains active research area\n",
    "- Long-context handling (DeltaNet, etc.) is emerging\n",
    "- Multi-token prediction gaining traction\n",
    "\n",
    "After all these years, LLM architecture development remains exciting!\n",
    "\n",
    "---\n",
    "\n",
    "*Based on Sebastian Raschka's \"The Big LLM Architecture Comparison\" (2025)*\n",
    "\n",
    "*This notebook is for educational purposes and revision. For the most up-to-date information, refer to the original article and model papers.*\n",
    "\n",
    "## Final Notes\n",
    "\n",
    "This notebook is designed to be your comprehensive guide for understanding modern LLM architectures. Use it as:\n",
    "\n",
    "✅ **Reference Material**: Quick lookup for architecture details  \n",
    "✅ **Study Guide**: Systematic learning path through concepts  \n",
    "✅ **Practice Resource**: Exercises and code examples  \n",
    "✅ **Revision Tool**: Checklist and quiz for testing knowledge  \n",
    "\n",
    "**Tips for Effective Use:**\n",
    "1. Don't try to memorize everything - understand the concepts\n",
    "2. Run the code examples and experiment with parameters\n",
    "3. Compare similar architectures to see patterns\n",
    "4. Read the original papers for deeper understanding\n",
    "5. Stay updated - the field evolves rapidly!\n",
    "\n",
    "**Remember**: The goal isn't to know every detail of every model, but to understand:\n",
    "- Why certain design choices are made\n",
    "- Trade-offs between different approaches\n",
    "- How to select and adapt architectures for your needs\n",
    "\n",
    "Good luck with your studies! 🚀\n",
    "\n",
    "---\n",
    "\n",
    "*Last Updated: Based on Sebastian Raschka's article through September 2025*\n",
    "\n",
    "*For the latest developments, visit: https://magazine.sebastianraschka.com/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scheduler_agent (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
