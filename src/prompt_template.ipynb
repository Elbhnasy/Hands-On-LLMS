{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914f999f",
   "metadata": {},
   "source": [
    "## Ollama \n",
    "* genration and chat completion\n",
    "    * [Ollama  completion refrence](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Get the API URL and model from the settings\n",
    "generation_api_url = \"http://localhost:11434/api/generate\"\n",
    "model = \"llama3.2:latest\"\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# Send the request to the Ollama API\n",
    "response = requests.post(\n",
    "    generation_api_url,\n",
    "    json={\n",
    "        \"model\": model,  # Use the model variable\n",
    "        \"prompt\": f\"Summarize the following text in 3 bullet points: {text}\",  # Fixed syntax\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "            \"repetition_penalty\": 1.2\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd2af79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.2:latest',\n",
       " 'created_at': '2025-05-01T16:50:36.442025314Z',\n",
       " 'response': \"There is no text to summarize. The provided text only contains a greeting. If you'd like to share the actual text, I'd be happy to help!\",\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'context': [128006,\n",
       "  9125,\n",
       "  128007,\n",
       "  271,\n",
       "  38766,\n",
       "  1303,\n",
       "  33025,\n",
       "  2696,\n",
       "  25,\n",
       "  6790,\n",
       "  220,\n",
       "  2366,\n",
       "  18,\n",
       "  271,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  9370,\n",
       "  5730,\n",
       "  553,\n",
       "  279,\n",
       "  2768,\n",
       "  1495,\n",
       "  304,\n",
       "  220,\n",
       "  18,\n",
       "  17889,\n",
       "  3585,\n",
       "  25,\n",
       "  22691,\n",
       "  11,\n",
       "  1268,\n",
       "  527,\n",
       "  499,\n",
       "  30,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  3947,\n",
       "  374,\n",
       "  912,\n",
       "  1495,\n",
       "  311,\n",
       "  63179,\n",
       "  13,\n",
       "  578,\n",
       "  3984,\n",
       "  1495,\n",
       "  1193,\n",
       "  5727,\n",
       "  264,\n",
       "  43213,\n",
       "  13,\n",
       "  1442,\n",
       "  499,\n",
       "  4265,\n",
       "  1093,\n",
       "  311,\n",
       "  4430,\n",
       "  279,\n",
       "  5150,\n",
       "  1495,\n",
       "  11,\n",
       "  358,\n",
       "  4265,\n",
       "  387,\n",
       "  6380,\n",
       "  311,\n",
       "  1520,\n",
       "  0],\n",
       " 'total_duration': 1901817504,\n",
       " 'load_duration': 19271281,\n",
       " 'prompt_eval_count': 43,\n",
       " 'prompt_eval_duration': 13908170,\n",
       " 'eval_count': 33,\n",
       " 'eval_duration': 1867861426}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0fa156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There is no text to summarize. The provided text only contains a greeting. If you'd like to share the actual text, I'd be happy to help!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae0ceba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There is no text to summarize. The provided text only contains a greeting. '\n",
      " \"If you'd like to share the actual text, I'd be happy to help!\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pprint\n",
    "pprint(response.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d934b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_settings\n",
    "from openai import OpenAI\n",
    "import os \n",
    "\n",
    "setting = get_settings()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = setting.OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "llm_model = setting.LLM_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caabebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer setup\n",
    "model_id = env_values['MODEL_ID']\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                quantization_config=quantization_config,\n",
    "                                                device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=base_model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=256,\n",
    "                truncation=True,  # Explicitly enable truncation\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2)\n",
    "\n",
    "# Initialize LangChain HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c57802",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Suggest 2 ways to lose my weight.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(llm.invoke(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "Suggest 2 ways to lose my weight.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_2 = \"\"\"\n",
    "Tell me a joke\n",
    "\"\"\".strip()\n",
    "\n",
    "llm_results = llm.generate([ prompt_1, prompt_2 ])\n",
    "llm_results.generations[1][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct pipeline usage\n",
    "pipeline_output = pipe(template, return_full_text=False)\n",
    "print(\"Pipeline Output:\", pipeline_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6b00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
